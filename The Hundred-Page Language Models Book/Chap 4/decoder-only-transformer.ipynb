{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf773fc-098f-428e-8817-89a25ce1c035",
   "metadata": {},
   "source": [
    "We're going to build, train, and evaluate a PyTorch-based decoder-only transformer to predict tokens. This is an exercise from Chapter 4 of **The Hundred Page Language Models Book**.\n",
    "\n",
    "Here is the architecture for a single decoder block:\n",
    "\n",
    "<img src=\"../images/transformer-decoder-block-architecture.png\" style=\"width:800px; border: 1px solid #000;\"/>\n",
    "\n",
    "Each decoder-only transformer model will be have 1 or more of these blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac274e5b-523f-4774-9d15-0d2fc98dc398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import tarfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # softmax\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a4f98-ae4a-4e15-9d57-ff101636b12e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0e2a5d-a4f9-47ad-8717-39eeeb71f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_decoder_language_model.ipynb\n",
    "#\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization\n",
    "    A simplified alternative to Layer Normalization that only uses RMS statistics\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))  # Learnable scale parameter\n",
    "        self.epsilon = epsilon  # Small constant for numerical stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute root mean square normalization\n",
    "        squared_x = x ** 2\n",
    "        mean_squared = torch.mean(squared_x, dim=-1, keepdim=True)\n",
    "        rms = torch.sqrt(mean_squared + self.epsilon)\n",
    "\n",
    "        # Normalize and scale\n",
    "        x_normalized = x / rms\n",
    "        output = x_normalized * self.scale\n",
    "        return output\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_decoder_language_model.ipynb\n",
    "#\n",
    "def rope(x, theta_base=10000.0):\n",
    "    \"\"\"\n",
    "    Implements Rotary Position Embedding (RoPE) for transformer attention.\n",
    "    RoPE encodes position information through rotation matrices applied to pairs of dimensions.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, seq_len, emb_dim)\n",
    "        theta_base: Base for computing rotation frequencies (default: 10000.0)\n",
    "\n",
    "    Returns:\n",
    "        Tensor with position information encoded through rotations\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, emb_dim = x.size()\n",
    "    assert emb_dim % 2 == 0, \"Embedding dimensionality must be even for RoPE\"\n",
    "\n",
    "    # Generate sequence position indices\n",
    "    pos = torch.arange(0, seq_len, dtype=torch.float32, device=x.device)\n",
    "    pos = pos.unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "    # Compute frequency bands for each dimension pair\n",
    "    # Modified: frequencies start from p=1 and use (p-1) in exponent\n",
    "    p = torch.arange(1, emb_dim // 2 + 1, dtype=torch.float32, device=x.device)\n",
    "    theta_p = 1.0 / (theta_base ** (2 * (p - 1) / emb_dim))\n",
    "\n",
    "    # Compute rotation angles for each position and frequency\n",
    "    pos = pos.unsqueeze(-1)\n",
    "    theta = pos * theta_p\n",
    "\n",
    "    # Compute rotation components\n",
    "    sin_theta = torch.sin(theta)\n",
    "    cos_theta = torch.cos(theta)\n",
    "\n",
    "    # Split input into alternating dimensions\n",
    "    x1 = x[..., 0::2]  # Dimensions at indices 0,2,4,...\n",
    "    x2 = x[..., 1::2]  # Dimensions at indices 1,3,5,...\n",
    "\n",
    "    # Apply 2D rotations to each pair\n",
    "    x_rotated_1 = x1 * cos_theta - x2 * sin_theta\n",
    "    x_rotated_2 = x1 * sin_theta + x2 * cos_theta\n",
    "\n",
    "    # Recombine rotated pairs into final output\n",
    "    x_rotated = torch.stack((x_rotated_1, x_rotated_2), dim=-1).reshape(batch_size, seq_len, emb_dim)\n",
    "\n",
    "    return x_rotated\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def check_file_exists(filename):\n",
    "    \"\"\"\n",
    "    Checks if a file exists in the current directory.\n",
    "    Args:\n",
    "        filename (str): Name of the file to check\n",
    "    Returns:\n",
    "        bool: True if file exists, False otherwise\n",
    "    \"\"\"\n",
    "    return os.path.exists(filename)\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def create_collate_fn(tokenizer):\n",
    "    \"\"\"\n",
    "    Creates a collate function for batching sequences of different lengths.\n",
    "    This function pads shorter sequences to match the longest sequence in the batch.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer object containing padding token information\n",
    "\n",
    "    Returns:\n",
    "        function: Collate function that handles padding in batches\n",
    "    \"\"\"\n",
    "    def collate_fn(batch):\n",
    "        # Separate inputs and targets from batch\n",
    "        input_seqs, target_seqs = zip(*batch)\n",
    "        # Get padding token ID from tokenizer\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        # Pad input sequences to same length\n",
    "        input_padded = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=pad_index)\n",
    "        # Pad target sequences to same length\n",
    "        target_padded = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=pad_index)\n",
    "        return input_padded, target_padded\n",
    "    return collate_fn\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def download_and_prepare_data(url, batch_size, tokenizer, max_length=30):\n",
    "    \"\"\"\n",
    "    Main function to handle the complete data preparation pipeline.\n",
    "    Downloads data, extracts it, and creates necessary dataset objects.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL where the dataset archive can be downloaded\n",
    "        batch_size (int): Batch size for data loading\n",
    "        tokenizer: Tokenizer object for text processing\n",
    "        max_length (int): Maximum sequence length for tokenization (default: 30)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataloader, test_dataloader) - Ready-to-use data loaders\n",
    "    \"\"\"\n",
    "    # Step 1: Download dataset archive from URL\n",
    "    filename = download_file(url)\n",
    "\n",
    "    # Step 2: Extract training and test files from archive\n",
    "    train_file, test_file = extract_dataset(filename)\n",
    "\n",
    "    # Step 3: Create dataset objects for streaming data\n",
    "    train_dataset, test_dataset = create_datasets(train_file, test_file, tokenizer, max_length)\n",
    "\n",
    "    # Step 4: Create function to handle batch creation\n",
    "    collate_fn = create_collate_fn(tokenizer)\n",
    "\n",
    "    # Step 5: Create and return data loaders\n",
    "    return create_dataloaders(train_dataset, test_dataset, batch_size, collate_fn)\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def download_file(url):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL if it doesn't exist locally.\n",
    "    Uses a custom User-Agent to help prevent download blocks.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the file to download\n",
    "    Returns:\n",
    "        str: Name of the downloaded file (\"news.tar.gz\")\n",
    "    \"\"\"\n",
    "    # Always use news.tar.gz as the filename, regardless of URL\n",
    "    filename = \"news.tar.gz\"\n",
    "\n",
    "    if not check_file_exists(filename):\n",
    "        print(f\"Downloading dataset from {url}...\")\n",
    "        req = urllib.request.Request(\n",
    "            url,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        )\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            with open(filename, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        print(\"Download completed.\")\n",
    "    else:\n",
    "        print(f\"{filename} already downloaded.\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def extract_dataset(filename):\n",
    "    \"\"\"\n",
    "    Extracts train.txt and test.txt from the downloaded archive.\n",
    "    Includes debug information about archive contents.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the archive file\n",
    "    Returns:\n",
    "        tuple: Paths to extracted train and test files\n",
    "    \"\"\"\n",
    "    data_dir = os.path.join(os.path.dirname(filename), \"news\")\n",
    "    train_path = os.path.join(data_dir, \"train.txt\")\n",
    "    test_path = os.path.join(data_dir, \"test.txt\")\n",
    "\n",
    "    if check_file_exists(train_path) and check_file_exists(test_path):\n",
    "        print(\"Data files already extracted.\")\n",
    "        return train_path, test_path\n",
    "\n",
    "    print(\"\\nListing archive contents:\")\n",
    "    with tarfile.open(filename, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            print(f\"Archive member: {member.name}\")\n",
    "\n",
    "        print(\"\\nExtracting files...\")\n",
    "        # Extract to current directory first\n",
    "        tar.extractall('.')\n",
    "\n",
    "    if not (check_file_exists(train_path) and check_file_exists(test_path)):\n",
    "        raise FileNotFoundError(f\"Required files not found in the archive. Please check the paths above.\")\n",
    "\n",
    "    print(\"Extraction completed.\")\n",
    "    return train_path, test_path\n",
    "\n",
    "    \n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def create_datasets(train_file, test_file, tokenizer, max_length=30):\n",
    "    \"\"\"\n",
    "    Creates IterableTextDataset objects for training and testing.\n",
    "    These datasets will stream data from disk instead of loading it all into memory.\n",
    "\n",
    "    Args:\n",
    "        train_file (str): Path to training data file\n",
    "        test_file (str): Path to test data file\n",
    "        tokenizer: Tokenizer object for text processing\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset) - Dataset objects for training and testing\n",
    "    \"\"\"\n",
    "    # Create training dataset\n",
    "    train_dataset = IterableTextDataset(train_file, tokenizer, max_length)\n",
    "    # Create test dataset\n",
    "    test_dataset = IterableTextDataset(test_file, tokenizer, max_length)\n",
    "\n",
    "    # Print dataset sizes\n",
    "    print(f\"Training sentences: {len(train_dataset)}\")\n",
    "    print(f\"Test sentences: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "    \n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def create_dataloaders(train_dataset, test_dataset, batch_size, collate_fn):\n",
    "    \"\"\"\n",
    "    Creates DataLoader objects for efficient data iteration.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: Training dataset\n",
    "        test_dataset: Test dataset\n",
    "        batch_size (int): Number of sequences per batch\n",
    "        collate_fn: Function to handle padding and batch creation\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataloader, test_dataloader) - DataLoader objects for\n",
    "               iterating over batches of data with proper padding\n",
    "    \"\"\"\n",
    "    # Create training data loader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,    # Function to handle padding\n",
    "        num_workers=0             # Number of worker processes (0 = single process)\n",
    "    )\n",
    "    # Create test data loader\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "class IterableTextDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    An iterable dataset for processing text data in a memory-efficient way.\n",
    "    Instead of loading all data into memory, it streams data from disk.\n",
    "    Inherits from PyTorch's IterableDataset for streaming support.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file containing sentences\n",
    "        tokenizer: Tokenizer object for converting text to tokens\n",
    "        max_length (int): Maximum sequence length to process (default: 30)\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, tokenizer, max_length=30):\n",
    "        # Store file path for reading data\n",
    "        self.file_path = file_path\n",
    "        # Store tokenizer for text processing\n",
    "        self.tokenizer = tokenizer\n",
    "        # Set maximum sequence length to truncate long sequences\n",
    "        self.max_length = max_length\n",
    "        self._count_sentences()\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Creates an iterator over the dataset.\n",
    "        This method is called when iterating over the dataset.\n",
    "\n",
    "        Yields:\n",
    "            tuple: (input_sequence, target_sequence) pairs for language modeling\n",
    "                  input_sequence is the sequence up to the last token\n",
    "                  target_sequence is the sequence shifted one position right\n",
    "        \"\"\"\n",
    "        # Open file in read mode with UTF-8 encoding\n",
    "        with open(self.file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            # Process each line (sentence) in the file\n",
    "            for line in f:\n",
    "                # Remove leading/trailing whitespace\n",
    "                sentence = line.strip()\n",
    "                # Replace all numbers with ### placeholder\n",
    "                # This reduces vocabulary size and helps model generalize\n",
    "                sentence = re.sub(r\"\\d+\", \"###\", sentence)\n",
    "\n",
    "                # Convert sentence to token IDs\n",
    "                encoded_sentence = self.tokenizer.encode(\n",
    "                    sentence,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True\n",
    "                )\n",
    "\n",
    "                # Only use sequences with at least 2 tokens\n",
    "                # (need at least one input and one target token)\n",
    "                if len(encoded_sentence) >= 2:\n",
    "                    # Input is all tokens except last\n",
    "                    input_seq = encoded_sentence[:-1]\n",
    "                    # Target is all tokens except first\n",
    "                    target_seq = encoded_sentence[1:]\n",
    "                    # Convert to PyTorch tensors and yield\n",
    "                    yield torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self._num_sentences\n",
    "\n",
    "    def _count_sentences(self):\n",
    "        print(f\"Counting sentences in {self.file_path}...\")\n",
    "        with open(self.file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            self._num_sentences = sum(1 for _ in f)\n",
    "        print(f\"Found {self._num_sentences} sentences in {self.file_path}.\")\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_decoder_language_model.ipynb\n",
    "#\n",
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Initialize the weights of different model components using appropriate schemes.\n",
    "    Each layer type receives specialized initialization for optimal training.\n",
    "    \"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Xavier uniform initialization for linear layers\n",
    "            # Helps maintain variance across network layers\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)  # Initialize biases to zero\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Initialize embedding layers with normal distribution\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                # Ensure padding tokens have zero embeddings\n",
    "                with torch.no_grad():\n",
    "                    module.weight[module.padding_idx].fill_(0)\n",
    "        elif isinstance(module, AttentionHead):\n",
    "            # Initialize query, key, and value projection matrices\n",
    "            # Xavier uniform helps maintain good gradient flow\n",
    "            nn.init.xavier_uniform_(module.W_Q)\n",
    "            nn.init.xavier_uniform_(module.W_K)\n",
    "            nn.init.xavier_uniform_(module.W_V)\n",
    "        elif isinstance(module, MultiHeadAttention):\n",
    "            # Initialize output projection matrix for attention mechanism\n",
    "            nn.init.xavier_uniform_(module.W_O)\n",
    "        elif isinstance(module, DecoderLanguageModel):\n",
    "            # Initialize final output projection layer\n",
    "            nn.init.xavier_uniform_(module.output)\n",
    "        elif isinstance(module, RMSNorm):\n",
    "            # Initialize RMSNorm scale parameters to ones\n",
    "            # This starts with identity transformation\n",
    "            nn.init.ones_(module.scale)\n",
    "        elif isinstance(module, MLP):\n",
    "            # Initialize feed-forward network parameters\n",
    "            nn.init.xavier_uniform_(module.W_1)\n",
    "            nn.init.xavier_uniform_(module.W_2)\n",
    "            nn.init.zeros_(module.B_1)\n",
    "            nn.init.zeros_(module.B_2)\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def compute_loss_and_perplexity(model, dataloader, tokenizer, criterion, device, max_sentences=1000):\n",
    "    \"\"\"\n",
    "    Evaluates model performance by computing loss and perplexity on data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The language model to evaluate\n",
    "        dataloader (DataLoader): Data loader containing batched sequences\n",
    "        tokenizer: Tokenizer for handling special tokens like padding\n",
    "        criterion: Loss function (usually CrossEntropyLoss)\n",
    "        device: Device to run computation on (cuda/cpu)\n",
    "        max_sentences (int): Maximum number of sentences to evaluate (default: 1000)\n",
    "                           Limits evaluation to a subset for faster validation\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, perplexity)\n",
    "               - average_loss: Mean loss per token (excluding padding)\n",
    "               - perplexity: exp(average_loss), lower is better\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode (disables dropout, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for loss calculation\n",
    "    total_loss = 0.0          # Accumulator for total loss across all batches\n",
    "    total_tokens = 0          # Counter for total number of tokens (excluding padding)\n",
    "    sentences_processed = 0    # Counter for number of sentences processed\n",
    "\n",
    "    # Disable gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        # Iterate through data with progress bar\n",
    "        for input_seq, target_seq in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            # Move input and target sequences to specified device\n",
    "            input_seq = input_seq.to(device)      # Shape: (batch_size, seq_len)\n",
    "            target_seq = target_seq.to(device)    # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Get current batch size (might be smaller for last batch)\n",
    "            batch_size_current = input_seq.size(0)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            logits = model(input_seq)             # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # Reshape logits and target for loss calculation\n",
    "            logits = logits.reshape(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n",
    "            target = target_seq.reshape(-1)              # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Create mask to exclude padding tokens\n",
    "            mask = target != tokenizer.pad_token_id\n",
    "\n",
    "            # Compute loss only on non-padded tokens\n",
    "            loss = criterion(logits[mask], target[mask])\n",
    "\n",
    "            # Update counters\n",
    "            loss_value = loss.item() * mask.sum().item()  # Total loss for this batch\n",
    "            total_loss += loss_value                      # Accumulate batch loss\n",
    "            total_tokens += mask.sum().item()             # Count non-padding tokens\n",
    "\n",
    "            # Update sentence counter and check if we've reached maximum\n",
    "            sentences_processed += batch_size_current\n",
    "            if sentences_processed >= max_sentences:\n",
    "                break\n",
    "\n",
    "    # Calculate final metrics\n",
    "    average_loss = total_loss / total_tokens           # Normalize loss by number of tokens\n",
    "    perplexity = math.exp(average_loss)               # Convert loss to perplexity\n",
    "\n",
    "    return average_loss, perplexity\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def generate_text(model, start_string, tokenizer, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Generates text continuation from a given start string using greedy decoding.\n",
    "    This method always chooses the most likely next token.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained language model\n",
    "        start_string (str): Initial text to continue from\n",
    "        tokenizer: Tokenizer for text processing\n",
    "        device: Device to run generation on (cuda/cpu)\n",
    "        max_length (int): Maximum length of generated sequence\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text continuation\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start string to token ids and move to device\n",
    "    # return_tensors=\"pt\" returns PyTorch tensor instead of list\n",
    "    tokens = tokenizer.encode(start_string, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n",
    "\n",
    "    # Initialize generated sequence with input tokens\n",
    "    generated = tokens\n",
    "\n",
    "    # Generate new tokens one at a time\n",
    "    for _ in range(max_length):\n",
    "        # Get model's predictions\n",
    "        output = model(generated)                    # Shape: (1, seq_len, vocab_size)\n",
    "        # Get logits for the next token (last position)\n",
    "        next_token_logits = output[0, -1, :]        # Shape: (vocab_size)\n",
    "\n",
    "        # Choose token with highest probability (greedy decoding)\n",
    "        # unsqueeze twice to match expected shape (1, 1)\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Add new token to generated sequence\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "\n",
    "        # Stop if end of sequence token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Convert token ids back to text\n",
    "    generated_text = tokenizer.decode(generated.squeeze().tolist())\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99db113-fdcc-4caf-b5c1-a5b8acdd102d",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "To build the model, we'll define:\n",
    "\n",
    "1. **a single attention head**, which is used to capture relationships between the input tokens\n",
    "2. **multi-head attention**, which is made up of multiple attention heads and concatenates and projects their output using trainable parameters\n",
    "3. **multilayer perceptron** (**MLP**), which transforms the output of self-attention to enable the model to learn complex data patterns\n",
    "4. **decoder block**, which combines multi-head attention, MLP, and introduces RMS normalization and residual connections\n",
    "5. **decoder language model**, which converts inputs to embeddings and is made up of 1 or more decoder blocks\n",
    "\n",
    "\n",
    "### Attention Head\n",
    "This is used to capture relations between input tokens. It does this by calculating **query** (`Q`), **key** (`K`), and **value** (`V`) matrices, along with separate tuned parameters (`W_Q`, `W_K`, and `W_V`, respectively). \n",
    "\n",
    "This also applies **Rotary position embedding** (**RoPE**) to apply position-dependent rotations to query and key vectors, in order to account for word order. Token embeddings closer together produce angles that are closer together than if they were further apart; furthermore, rotations at the start of the input start out larger and decrease the further along the input.\n",
    "\n",
    "The `mask` is a **causal mask** that prevents an embedding later in the input from influencing the attention score:\n",
    "\n",
    "```\n",
    "M = [ 1 -∞ -∞ -∞\n",
    "      1  1 -∞ -∞\n",
    "      1  1  1 -∞\n",
    "      1  1  1  1 ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920f3909-81e1-4e7b-88e4-02d64c4cfbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, emb_dim, d_h):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Parameter(torch.empty(emb_dim, d_h))\n",
    "        self.W_K = nn.Parameter(torch.empty(emb_dim, d_h))\n",
    "        self.W_V = nn.Parameter(torch.empty(emb_dim, d_h))\n",
    "        self.d_h = d_h\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        Q = x @ self.W_Q\n",
    "        K = x @ self.W_K\n",
    "        V = x @ self.W_V\n",
    "        Q, K = rope(Q), rope(K)\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_h)\n",
    "        masked_scores = scores.masked_fill(mask == 0, float(\"-inf\")) # apply mask, converting '0's to '-∞'s\n",
    "        attention_weights = torch.softmax(masked_scores, dim=-1)     # produce attention weights (logits) from scores across last dimension\n",
    "        \n",
    "        return attention_weights @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b052c-840c-432f-9f2d-798e4a840536",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Made up of multiple attention heads and concatenates and projects their output using trainable parameters (*projection matrix*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec4e13e-84a6-4ca7-a327-16b1b37340a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        d_h = emb_dim // num_heads   # dimensionality of each head\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(emb_dim, d_h) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.W_O = nn.Parameter(torch.empty(emb_dim, emb_dim)) # projection matrix\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        head_outputs = [head(x, mask) for head in self.heads]\n",
    "        x = torch.cat(head_outputs, dim=-1)\n",
    "        return x @ self.W_O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fae82-2a82-42fd-8145-1666fa11006a",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP)\n",
    "\n",
    "Transforms the output of self-attention to enable the model to learn complex data patterns. \n",
    "\n",
    "This is like a typical feedforward network layer, except that the transformation applies additional parameters (`W_2` and `B_2`) after the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7958600c-89b4-4c86-bf2f-8197ec8dc4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.W_1 = nn.Parameter(torch.empty(emb_dim, emb_dim * 4))\n",
    "        self.B_1 = nn.Parameter(torch.empty(emb_dim * 4))\n",
    "        self.W_2 = nn.Parameter(torch.empty(emb_dim * 4, emb_dim))\n",
    "        self.B_2 = nn.Parameter(torch.empty(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ self.W_1 + self.B_1\n",
    "        x = torch.relu(x)\n",
    "        return x @ self.W_2 + self.B_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748715b-ec10-4cb4-bdb9-02a2759e8f6c",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "\n",
    "This combines multi-head attention, MLP, and introduces RMS normalization and residual connections. \n",
    "\n",
    "**Root Mean Square normalization** is used to keep the scale of inputs to each layer consistent, preventing gradients from becoming excessively large or small (improving \"numerical stability\"). Note each RMS layer contains trainable parameters, hence multiple `RMSNorm` instances.\n",
    "\n",
    "**Residual connections** is used to address the vanishing gradient problem. (The input `x` is applied to the output. Mathematically, this prevents the gradient of earlier layers from approaching 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd941cb3-3172-4650-9485-d69e5e42d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(emb_dim)\n",
    "        self.attn = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.norm2 = RMSNorm(emb_dim)\n",
    "        self.mlp = MLP(emb_dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_out = self.attn(self.norm1(x), mask)\n",
    "        x = x + attn_out  # apply residual connection\n",
    "        mlp_out = self.mlp(self.norm2(x))\n",
    "        x = x + mlp_out  # apply residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9187607-f76b-43d3-99a1-5054bc99c814",
   "metadata": {},
   "source": [
    "### Decoder Language Model\n",
    "\n",
    "This is the final model. It converts inputs to embeddings and is made up of 1 or more decoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ede332-0d8c-48d7-820f-cca409f8ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_heads, num_blocks, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(emb_dim, num_heads) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.output = nn.Parameter(torch.rand(emb_dim, vocab_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, seq_len, _ = x.shape\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))  # create the causal mask applied during multihead attention\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x @ self.output  # apply one final tuneable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffd377-09a3-4967-81de-fa7834a2be9b",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67566a85-8ef1-4593-9c64-ffe224c653f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "emb_dim = 128\n",
    "num_heads = 8\n",
    "num_blocks = 2\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "context_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "571a2e28-4b35-48b2-bfee-230f5e0ef222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# find the device to run training on (hopefully GPU...)\n",
    "def get_device_label():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "#device = torch.device(get_device_label())\n",
    "device = torch.device(\"cpu\") # cpu faster than mps, tweak batch_size?\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# verify\n",
    "x = torch.ones(1, device=device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62310652-c597-4e1c-b519-259077229743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 32011\n",
      "\n",
      "CPU times: user 54.3 ms, sys: 27.1 ms, total: 81.3 ms\n",
      "Wall time: 262 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "pad_idx = tokenizer.pad_token_id  # used to pad shorter inputs\n",
    "vocab_size = len(tokenizer)\n",
    "print(f\"\\nVocabulary size: {vocab_size}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd7777ee-8465-4c5a-bb50-e3b3b659c8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news.tar.gz already downloaded.\n",
      "Data files already extracted.\n",
      "Counting sentences in news/train.txt...\n",
      "Found 22034911 sentences in news/train.txt.\n",
      "Counting sentences in news/test.txt...\n",
      "Found 449693 sentences in news/test.txt.\n",
      "Training sentences: 22034911\n",
      "Test sentences: 449693\n",
      "CPU times: user 2.97 s, sys: 407 ms, total: 3.38 s\n",
      "Wall time: 4.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_url = \"https://www.thelmbook.com/data/news\"\n",
    "train_dataloader, test_dataloader = download_and_prepare_data(\n",
    "    data_url, batch_size, tokenizer, context_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68c5adbe-e232-4bf3-8d40-eed5c36baf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total trainable parameters: 8589824\n",
      "\n",
      "[epoch=0] batch 0 of 172148 - 0.00% done - 0:00:00.008975 ellapsed, est. total time: unknown\n",
      "[epoch=0] batch 172 of 172148 - 0.10% done - 0:01:29.935767 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 344 of 172148 - 0.20% done - 0:02:59.453377 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 516 of 172148 - 0.30% done - 0:04:29.481362 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 688 of 172148 - 0.40% done - 0:06:04.502384 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 860 of 172148 - 0.50% done - 0:07:38.608208 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 1032 of 172148 - 0.60% done - 0:09:13.259641 ellapsed, est. total time: 25.6 hr\n",
      "[epoch=0] batch 1204 of 172148 - 0.70% done - 0:10:46.948874 ellapsed, est. total time: 25.7 hr\n",
      "[epoch=0] batch 1376 of 172148 - 0.80% done - 0:12:20.584162 ellapsed, est. total time: 25.7 hr\n",
      "[epoch=0] batch 1548 of 172148 - 0.90% done - 0:13:55.248231 ellapsed, est. total time: 25.8 hr\n",
      "[epoch=0] batch 1720 of 172148 - 1.00% done - 0:15:25.984609 ellapsed, est. total time: 25.7 hr\n",
      "[epoch=0] batch 1892 of 172148 - 1.10% done - 0:16:56.606323 ellapsed, est. total time: 25.7 hr\n",
      "[epoch=0] batch 2064 of 172148 - 1.20% done - 0:18:28.309198 ellapsed, est. total time: 25.7 hr\n",
      "[epoch=0] batch 2236 of 172148 - 1.30% done - 0:20:01.072618 ellapsed, est. total time: 25.7 hr\n",
      "[epoch=0] batch 2408 of 172148 - 1.40% done - 0:21:31.198547 ellapsed, est. total time: 25.6 hr\n",
      "[epoch=0] batch 2580 of 172148 - 1.50% done - 0:23:01.508320 ellapsed, est. total time: 25.6 hr\n",
      "[epoch=0] batch 2752 of 172148 - 1.60% done - 0:24:31.301125 ellapsed, est. total time: 25.6 hr\n",
      "[epoch=0] batch 2924 of 172148 - 1.70% done - 0:26:00.933597 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 3096 of 172148 - 1.80% done - 0:27:30.511404 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 3268 of 172148 - 1.90% done - 0:29:00.465341 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 3440 of 172148 - 2.00% done - 0:30:29.866951 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 3612 of 172148 - 2.10% done - 0:32:00.273667 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 3784 of 172148 - 2.20% done - 0:33:29.875318 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 3956 of 172148 - 2.30% done - 0:35:00.361564 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 4128 of 172148 - 2.40% done - 0:36:30.594384 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 4300 of 172148 - 2.50% done - 0:38:00.546743 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 4472 of 172148 - 2.60% done - 0:39:30.404626 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 4644 of 172148 - 2.70% done - 0:41:00.198548 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 4816 of 172148 - 2.80% done - 0:42:29.598537 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 4988 of 172148 - 2.90% done - 0:43:59.427442 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 5160 of 172148 - 3.00% done - 0:45:29.077923 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 5332 of 172148 - 3.10% done - 0:46:59.131124 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 5504 of 172148 - 3.20% done - 0:48:28.988679 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 5676 of 172148 - 3.30% done - 0:50:06.269805 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 5848 of 172148 - 3.40% done - 0:51:45.107185 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 6020 of 172148 - 3.50% done - 0:53:20.999094 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 6192 of 172148 - 3.60% done - 0:54:50.276616 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 6364 of 172148 - 3.70% done - 0:56:21.508185 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 6536 of 172148 - 3.80% done - 0:57:52.093259 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 6708 of 172148 - 3.90% done - 0:59:22.503409 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 6880 of 172148 - 4.00% done - 1:00:56.011580 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 7052 of 172148 - 4.10% done - 1:02:31.138466 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 7224 of 172148 - 4.20% done - 1:04:03.735251 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 7396 of 172148 - 4.30% done - 1:05:37.415841 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 7568 of 172148 - 4.40% done - 1:07:09.954234 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 7740 of 172148 - 4.50% done - 1:08:42.494180 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 7912 of 172148 - 4.60% done - 1:10:14.844042 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 8084 of 172148 - 4.70% done - 1:11:47.797068 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 8256 of 172148 - 4.80% done - 1:13:15.597860 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 8428 of 172148 - 4.90% done - 1:14:44.312405 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 8600 of 172148 - 5.00% done - 1:16:13.069458 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 8772 of 172148 - 5.10% done - 1:17:42.024269 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 8944 of 172148 - 5.20% done - 1:19:11.324485 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 9116 of 172148 - 5.30% done - 1:20:40.948226 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 9288 of 172148 - 5.40% done - 1:22:11.138923 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 9460 of 172148 - 5.50% done - 1:23:41.316491 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 9632 of 172148 - 5.60% done - 1:25:09.730646 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 9804 of 172148 - 5.70% done - 1:26:38.657588 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 9976 of 172148 - 5.80% done - 1:28:07.249597 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 10148 of 172148 - 5.89% done - 1:29:36.949401 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 10320 of 172148 - 5.99% done - 1:31:05.976537 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 10492 of 172148 - 6.09% done - 1:32:35.116625 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 10664 of 172148 - 6.19% done - 1:34:03.820087 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 10836 of 172148 - 6.29% done - 1:35:32.499397 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 11008 of 172148 - 6.39% done - 1:37:01.635022 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 11180 of 172148 - 6.49% done - 1:38:40.519608 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 11352 of 172148 - 6.59% done - 1:40:19.480670 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 11524 of 172148 - 6.69% done - 1:41:51.429279 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 11696 of 172148 - 6.79% done - 1:43:23.308713 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 11868 of 172148 - 6.89% done - 1:44:55.792044 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 12040 of 172148 - 6.99% done - 1:46:28.058244 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 12212 of 172148 - 7.09% done - 1:48:00.061478 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 12384 of 172148 - 7.19% done - 1:49:33.545149 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 12556 of 172148 - 7.29% done - 1:51:04.729841 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 12728 of 172148 - 7.39% done - 1:52:32.925764 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 12900 of 172148 - 7.49% done - 1:54:04.408426 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 13072 of 172148 - 7.59% done - 1:55:38.963489 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 13244 of 172148 - 7.69% done - 1:57:07.865299 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 13416 of 172148 - 7.79% done - 1:58:36.304509 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 13588 of 172148 - 7.89% done - 2:00:04.552784 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 13760 of 172148 - 7.99% done - 2:01:33.294372 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 13932 of 172148 - 8.09% done - 2:03:02.331928 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 14104 of 172148 - 8.19% done - 2:04:37.024505 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 14276 of 172148 - 8.29% done - 2:06:10.327903 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 14448 of 172148 - 8.39% done - 2:07:43.104357 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 14620 of 172148 - 8.49% done - 2:09:16.427966 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 14792 of 172148 - 8.59% done - 2:10:49.394483 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 14964 of 172148 - 8.69% done - 2:12:22.860935 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 15136 of 172148 - 8.79% done - 2:13:56.073133 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 15308 of 172148 - 8.89% done - 2:15:29.576413 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 15480 of 172148 - 8.99% done - 2:17:03.102859 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 15652 of 172148 - 9.09% done - 2:18:37.250091 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 15824 of 172148 - 9.19% done - 2:20:12.937383 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 15996 of 172148 - 9.29% done - 2:21:46.922217 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 16168 of 172148 - 9.39% done - 2:23:16.147337 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 16340 of 172148 - 9.49% done - 2:24:46.830736 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 16512 of 172148 - 9.59% done - 2:26:16.692599 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 16684 of 172148 - 9.69% done - 2:27:46.532829 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 16856 of 172148 - 9.79% done - 2:29:16.415251 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 17028 of 172148 - 9.89% done - 2:30:46.096498 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 17200 of 172148 - 9.99% done - 2:32:15.428900 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 17372 of 172148 - 10.09% done - 2:33:44.604242 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 17544 of 172148 - 10.19% done - 2:35:13.779203 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 17716 of 172148 - 10.29% done - 2:36:42.466344 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 17888 of 172148 - 10.39% done - 2:38:11.277169 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 18060 of 172148 - 10.49% done - 2:39:39.966635 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 18232 of 172148 - 10.59% done - 2:41:08.495976 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 18404 of 172148 - 10.69% done - 2:42:37.582942 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 18576 of 172148 - 10.79% done - 2:44:14.895591 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 18748 of 172148 - 10.89% done - 2:45:55.907972 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 18920 of 172148 - 10.99% done - 2:47:34.993731 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 19092 of 172148 - 11.09% done - 2:49:14.168676 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 19264 of 172148 - 11.19% done - 2:50:58.017860 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 19436 of 172148 - 11.29% done - 2:52:30.626558 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 19608 of 172148 - 11.39% done - 2:54:03.251813 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 19780 of 172148 - 11.49% done - 2:55:36.102825 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 19952 of 172148 - 11.59% done - 2:57:09.016273 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 20124 of 172148 - 11.69% done - 2:58:42.271996 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 20296 of 172148 - 11.79% done - 3:00:14.941686 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 20468 of 172148 - 11.89% done - 3:01:47.218546 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 20640 of 172148 - 11.99% done - 3:03:15.690344 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 20812 of 172148 - 12.09% done - 3:04:44.720117 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 20984 of 172148 - 12.19% done - 3:06:14.047300 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 21156 of 172148 - 12.29% done - 3:07:43.483006 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 21328 of 172148 - 12.39% done - 3:09:12.224984 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 21500 of 172148 - 12.49% done - 3:10:40.980531 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 21672 of 172148 - 12.59% done - 3:12:09.762522 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 21844 of 172148 - 12.69% done - 3:13:38.621580 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 22016 of 172148 - 12.79% done - 3:15:07.146159 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 22188 of 172148 - 12.89% done - 3:16:35.957884 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 22360 of 172148 - 12.99% done - 3:18:04.538721 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 22532 of 172148 - 13.09% done - 3:19:34.056975 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 22704 of 172148 - 13.19% done - 3:21:03.239679 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 22876 of 172148 - 13.29% done - 3:22:31.961647 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 23048 of 172148 - 13.39% done - 3:24:01.339570 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 23220 of 172148 - 13.49% done - 3:25:30.777777 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 23392 of 172148 - 13.59% done - 3:27:15.085346 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 23564 of 172148 - 13.69% done - 3:28:59.542361 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 23736 of 172148 - 13.79% done - 3:30:35.056011 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 23908 of 172148 - 13.89% done - 3:32:08.645260 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 24080 of 172148 - 13.99% done - 3:33:41.897597 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 24252 of 172148 - 14.09% done - 3:35:15.496865 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 24424 of 172148 - 14.19% done - 3:36:48.850254 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 24596 of 172148 - 14.29% done - 3:38:21.746877 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 24768 of 172148 - 14.39% done - 3:39:54.915927 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 24940 of 172148 - 14.49% done - 3:41:25.170038 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 25112 of 172148 - 14.59% done - 3:42:53.779600 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 25284 of 172148 - 14.69% done - 3:44:22.446120 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 25456 of 172148 - 14.79% done - 3:45:53.558272 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 25628 of 172148 - 14.89% done - 3:47:22.031116 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 25800 of 172148 - 14.99% done - 3:48:50.608446 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 25972 of 172148 - 15.09% done - 3:50:20.459117 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 26144 of 172148 - 15.19% done - 3:51:55.080741 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 26316 of 172148 - 15.29% done - 3:53:28.774150 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 26488 of 172148 - 15.39% done - 3:55:02.950138 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 26660 of 172148 - 15.49% done - 3:56:35.911975 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 26832 of 172148 - 15.59% done - 3:58:08.680865 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 27004 of 172148 - 15.69% done - 3:59:41.567576 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 27176 of 172148 - 15.79% done - 4:01:15.545871 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 27348 of 172148 - 15.89% done - 4:02:48.835320 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 27520 of 172148 - 15.99% done - 4:04:22.039464 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 27692 of 172148 - 16.09% done - 4:05:49.860410 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 27864 of 172148 - 16.19% done - 4:07:18.300883 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 28036 of 172148 - 16.29% done - 4:08:46.512680 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 28208 of 172148 - 16.39% done - 4:10:15.095842 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 28380 of 172148 - 16.49% done - 4:11:43.406255 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 28552 of 172148 - 16.59% done - 4:13:16.187221 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 28724 of 172148 - 16.69% done - 4:15:01.956560 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 28896 of 172148 - 16.79% done - 4:16:44.419068 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 29068 of 172148 - 16.89% done - 4:18:24.789943 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 29240 of 172148 - 16.99% done - 4:20:04.724901 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 29412 of 172148 - 17.09% done - 4:21:41.298440 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 29584 of 172148 - 17.19% done - 4:23:15.006809 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 29756 of 172148 - 17.29% done - 4:24:47.493832 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 29928 of 172148 - 17.39% done - 4:26:19.623177 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 30100 of 172148 - 17.48% done - 4:27:52.725071 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 30272 of 172148 - 17.58% done - 4:29:25.431207 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 30444 of 172148 - 17.68% done - 4:30:58.903023 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 30616 of 172148 - 17.78% done - 4:32:27.543997 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 30788 of 172148 - 17.88% done - 4:33:56.561482 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 30960 of 172148 - 17.98% done - 4:35:25.210780 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 31132 of 172148 - 18.08% done - 4:36:53.722908 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 31304 of 172148 - 18.18% done - 4:38:22.066435 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 31476 of 172148 - 18.28% done - 4:39:50.855085 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 31648 of 172148 - 18.38% done - 4:41:18.998722 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 31820 of 172148 - 18.48% done - 4:42:47.353725 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 31992 of 172148 - 18.58% done - 4:44:15.765362 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 32164 of 172148 - 18.68% done - 4:45:44.280721 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 32336 of 172148 - 18.78% done - 4:47:12.667435 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 32508 of 172148 - 18.88% done - 4:48:41.252606 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 32680 of 172148 - 18.98% done - 4:50:09.375741 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 32852 of 172148 - 19.08% done - 4:51:37.779051 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 33024 of 172148 - 19.18% done - 4:53:06.035859 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 33196 of 172148 - 19.28% done - 4:54:34.746049 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 33368 of 172148 - 19.38% done - 4:56:03.134331 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 33540 of 172148 - 19.48% done - 4:57:31.619004 ellapsed, est. total time: 25.5 hr\n",
      "[epoch=0] batch 33712 of 172148 - 19.58% done - 4:59:00.225926 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 33884 of 172148 - 19.68% done - 5:00:29.097344 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 34056 of 172148 - 19.78% done - 5:02:01.281068 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 34228 of 172148 - 19.88% done - 5:03:30.697329 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 34400 of 172148 - 19.98% done - 5:05:00.220858 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 34572 of 172148 - 20.08% done - 5:06:29.742088 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 34744 of 172148 - 20.18% done - 5:07:59.384870 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 34916 of 172148 - 20.28% done - 5:09:29.275668 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 35088 of 172148 - 20.38% done - 5:10:58.996973 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 35260 of 172148 - 20.48% done - 5:12:32.171581 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 35432 of 172148 - 20.58% done - 5:14:00.721547 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 35604 of 172148 - 20.68% done - 5:15:29.077464 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 35776 of 172148 - 20.78% done - 5:16:57.565479 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 35948 of 172148 - 20.88% done - 5:18:26.479313 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 36120 of 172148 - 20.98% done - 5:19:55.523401 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 36292 of 172148 - 21.08% done - 5:21:23.987941 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 36464 of 172148 - 21.18% done - 5:22:52.398085 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 36636 of 172148 - 21.28% done - 5:24:20.813668 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 36808 of 172148 - 21.38% done - 5:25:49.286951 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 36980 of 172148 - 21.48% done - 5:27:17.803237 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 37152 of 172148 - 21.58% done - 5:28:46.325666 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 37324 of 172148 - 21.68% done - 5:30:14.866925 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 37496 of 172148 - 21.78% done - 5:31:43.651033 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 37668 of 172148 - 21.88% done - 5:33:11.812494 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 37840 of 172148 - 21.98% done - 5:34:40.518145 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 38012 of 172148 - 22.08% done - 5:36:09.721809 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 38184 of 172148 - 22.18% done - 5:37:38.018997 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 38356 of 172148 - 22.28% done - 5:39:06.788989 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 38528 of 172148 - 22.38% done - 5:40:35.313092 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 38700 of 172148 - 22.48% done - 5:42:03.530656 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 38872 of 172148 - 22.58% done - 5:43:31.813026 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 39044 of 172148 - 22.68% done - 5:45:00.358687 ellapsed, est. total time: 25.4 hr\n",
      "[epoch=0] batch 39216 of 172148 - 22.78% done - 5:46:28.483305 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 39388 of 172148 - 22.88% done - 5:47:56.735086 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 39560 of 172148 - 22.98% done - 5:49:26.100660 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 39732 of 172148 - 23.08% done - 5:50:55.102094 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 39904 of 172148 - 23.18% done - 5:52:23.372044 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 40076 of 172148 - 23.28% done - 5:53:51.835517 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 40248 of 172148 - 23.38% done - 5:55:20.306076 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 40420 of 172148 - 23.48% done - 5:56:48.767984 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 40592 of 172148 - 23.58% done - 5:58:17.071220 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 40764 of 172148 - 23.68% done - 5:59:45.752744 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 40936 of 172148 - 23.78% done - 6:01:15.136246 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 41108 of 172148 - 23.88% done - 6:02:45.290423 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 41280 of 172148 - 23.98% done - 6:04:13.957789 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 41452 of 172148 - 24.08% done - 6:05:42.615241 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 41624 of 172148 - 24.18% done - 6:07:11.110626 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 41796 of 172148 - 24.28% done - 6:08:39.772250 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 41968 of 172148 - 24.38% done - 6:10:08.135112 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 42140 of 172148 - 24.48% done - 6:11:36.383137 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 42312 of 172148 - 24.58% done - 6:13:04.603792 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 42484 of 172148 - 24.68% done - 6:14:33.141984 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 42656 of 172148 - 24.78% done - 6:16:01.344353 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 42828 of 172148 - 24.88% done - 6:17:29.662426 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 43000 of 172148 - 24.98% done - 6:18:58.505333 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 43172 of 172148 - 25.08% done - 6:20:26.683085 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 43344 of 172148 - 25.18% done - 6:21:56.171773 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 43516 of 172148 - 25.28% done - 6:23:24.311190 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 43688 of 172148 - 25.38% done - 6:24:53.202185 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 43860 of 172148 - 25.48% done - 6:26:21.417231 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 44032 of 172148 - 25.58% done - 6:27:49.815242 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 44204 of 172148 - 25.68% done - 6:29:18.292505 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 44376 of 172148 - 25.78% done - 6:30:46.807502 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 44548 of 172148 - 25.88% done - 6:32:15.213115 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 44720 of 172148 - 25.98% done - 6:33:43.637382 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 44892 of 172148 - 26.08% done - 6:35:11.944191 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 45064 of 172148 - 26.18% done - 6:36:40.095471 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 45236 of 172148 - 26.28% done - 6:38:08.346242 ellapsed, est. total time: 25.3 hr\n",
      "[epoch=0] batch 45408 of 172148 - 26.38% done - 6:39:36.409093 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 45580 of 172148 - 26.48% done - 6:41:04.543352 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 45752 of 172148 - 26.58% done - 6:42:32.746955 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 45924 of 172148 - 26.68% done - 6:44:01.301969 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 46096 of 172148 - 26.78% done - 6:45:29.499826 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 46268 of 172148 - 26.88% done - 6:46:58.121623 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 46440 of 172148 - 26.98% done - 6:48:26.250656 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 46612 of 172148 - 27.08% done - 6:49:55.861396 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 46784 of 172148 - 27.18% done - 6:51:24.457055 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 46956 of 172148 - 27.28% done - 6:52:53.185571 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 47128 of 172148 - 27.38% done - 6:54:22.089399 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 47300 of 172148 - 27.48% done - 6:55:51.203641 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 47472 of 172148 - 27.58% done - 6:57:19.745912 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 47644 of 172148 - 27.68% done - 6:58:48.229680 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 47816 of 172148 - 27.78% done - 7:00:17.258818 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 47988 of 172148 - 27.88% done - 7:01:47.400390 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 48160 of 172148 - 27.98% done - 7:03:16.009778 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 48332 of 172148 - 28.08% done - 7:04:44.294011 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 48504 of 172148 - 28.18% done - 7:06:12.469826 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 48676 of 172148 - 28.28% done - 7:07:40.629780 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 48848 of 172148 - 28.38% done - 7:09:09.327637 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 49020 of 172148 - 28.48% done - 7:10:37.956934 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 49192 of 172148 - 28.58% done - 7:12:06.221120 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 49364 of 172148 - 28.68% done - 7:13:34.629875 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 49536 of 172148 - 28.78% done - 7:15:03.265002 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 49708 of 172148 - 28.88% done - 7:16:31.369522 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 49880 of 172148 - 28.98% done - 7:17:59.728830 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 50052 of 172148 - 29.07% done - 7:19:28.881133 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 50224 of 172148 - 29.17% done - 7:20:57.112465 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 50396 of 172148 - 29.27% done - 7:22:25.300544 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 50568 of 172148 - 29.37% done - 7:23:53.628243 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 50740 of 172148 - 29.47% done - 7:25:21.861642 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 50912 of 172148 - 29.57% done - 7:26:50.206249 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 51084 of 172148 - 29.67% done - 7:28:18.722408 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 51256 of 172148 - 29.77% done - 7:29:47.262065 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 51428 of 172148 - 29.87% done - 7:31:15.712026 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 51600 of 172148 - 29.97% done - 7:32:44.297911 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 51772 of 172148 - 30.07% done - 7:34:14.186582 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 51944 of 172148 - 30.17% done - 7:35:42.887039 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 52116 of 172148 - 30.27% done - 7:37:11.348140 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 52288 of 172148 - 30.37% done - 7:38:40.250926 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 52460 of 172148 - 30.47% done - 7:40:09.106708 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 52632 of 172148 - 30.57% done - 7:41:37.773466 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 52804 of 172148 - 30.67% done - 7:43:06.223742 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 52976 of 172148 - 30.77% done - 7:44:34.820972 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 53148 of 172148 - 30.87% done - 7:46:02.894990 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 53320 of 172148 - 30.97% done - 7:47:31.134127 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 53492 of 172148 - 31.07% done - 7:48:59.405556 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 53664 of 172148 - 31.17% done - 7:50:28.353016 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 53836 of 172148 - 31.27% done - 7:51:56.585880 ellapsed, est. total time: 25.2 hr\n",
      "[epoch=0] batch 54008 of 172148 - 31.37% done - 7:53:24.221376 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 54180 of 172148 - 31.47% done - 7:54:52.541453 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 54352 of 172148 - 31.57% done - 7:56:20.706927 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 54524 of 172148 - 31.67% done - 7:57:48.941040 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 54696 of 172148 - 31.77% done - 7:59:17.184010 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 54868 of 172148 - 31.87% done - 8:00:45.993752 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 55040 of 172148 - 31.97% done - 8:02:14.360921 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 55212 of 172148 - 32.07% done - 8:03:42.499580 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 55384 of 172148 - 32.17% done - 8:05:10.874543 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 55556 of 172148 - 32.27% done - 8:06:40.971528 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 55728 of 172148 - 32.37% done - 8:08:09.270377 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 55900 of 172148 - 32.47% done - 8:09:37.765274 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 56072 of 172148 - 32.57% done - 8:11:05.926210 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 56244 of 172148 - 32.67% done - 8:12:33.848237 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 56416 of 172148 - 32.77% done - 8:14:02.837768 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 56588 of 172148 - 32.87% done - 8:15:31.145833 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 56760 of 172148 - 32.97% done - 8:16:59.000980 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 56932 of 172148 - 33.07% done - 8:18:26.979314 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 57104 of 172148 - 33.17% done - 8:19:56.500731 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 57276 of 172148 - 33.27% done - 8:21:25.671753 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 57448 of 172148 - 33.37% done - 8:22:54.348085 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 57620 of 172148 - 33.47% done - 8:24:22.902291 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 57792 of 172148 - 33.57% done - 8:25:51.139791 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 57964 of 172148 - 33.67% done - 8:27:20.106677 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 58136 of 172148 - 33.77% done - 8:28:48.643343 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 58308 of 172148 - 33.87% done - 8:30:17.408508 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 58480 of 172148 - 33.97% done - 8:31:46.143552 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 58652 of 172148 - 34.07% done - 8:33:14.124703 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 58824 of 172148 - 34.17% done - 8:34:42.569434 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 58996 of 172148 - 34.27% done - 8:36:10.824713 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 59168 of 172148 - 34.37% done - 8:37:38.947682 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 59340 of 172148 - 34.47% done - 8:39:07.191498 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 59512 of 172148 - 34.57% done - 8:40:35.270151 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 59684 of 172148 - 34.67% done - 8:42:03.270426 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 59856 of 172148 - 34.77% done - 8:43:31.699930 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 60028 of 172148 - 34.87% done - 8:44:59.992999 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 60200 of 172148 - 34.97% done - 8:46:27.895245 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 60372 of 172148 - 35.07% done - 8:47:55.504219 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 60544 of 172148 - 35.17% done - 8:49:23.688514 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 60716 of 172148 - 35.27% done - 8:50:51.514399 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 60888 of 172148 - 35.37% done - 8:52:19.480194 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 61060 of 172148 - 35.47% done - 8:53:47.844135 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 61232 of 172148 - 35.57% done - 8:55:15.904509 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 61404 of 172148 - 35.67% done - 8:56:44.110174 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 61576 of 172148 - 35.77% done - 8:58:12.151167 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 61748 of 172148 - 35.87% done - 8:59:40.551711 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 61920 of 172148 - 35.97% done - 9:01:08.983906 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 62092 of 172148 - 36.07% done - 9:02:37.696971 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 62264 of 172148 - 36.17% done - 9:04:07.250141 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 62436 of 172148 - 36.27% done - 9:05:35.914505 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 62608 of 172148 - 36.37% done - 9:07:04.509973 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 62780 of 172148 - 36.47% done - 9:08:33.387907 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 62952 of 172148 - 36.57% done - 9:10:02.095772 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 63124 of 172148 - 36.67% done - 9:11:31.079431 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 63296 of 172148 - 36.77% done - 9:12:59.659999 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 63468 of 172148 - 36.87% done - 9:14:28.247771 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 63640 of 172148 - 36.97% done - 9:15:57.279217 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 63812 of 172148 - 37.07% done - 9:17:26.027223 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 63984 of 172148 - 37.17% done - 9:18:55.131399 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 64156 of 172148 - 37.27% done - 9:20:24.463866 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 64328 of 172148 - 37.37% done - 9:21:53.052561 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 64500 of 172148 - 37.47% done - 9:23:21.877578 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 64672 of 172148 - 37.57% done - 9:24:50.737315 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 64844 of 172148 - 37.67% done - 9:26:19.954647 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 65016 of 172148 - 37.77% done - 9:27:48.664070 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 65188 of 172148 - 37.87% done - 9:29:17.525082 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 65360 of 172148 - 37.97% done - 9:30:46.472734 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 65532 of 172148 - 38.07% done - 9:32:14.505058 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 65704 of 172148 - 38.17% done - 9:33:42.290142 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 65876 of 172148 - 38.27% done - 9:35:10.273183 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 66048 of 172148 - 38.37% done - 9:36:39.552287 ellapsed, est. total time: 25.1 hr\n",
      "[epoch=0] batch 66220 of 172148 - 38.47% done - 9:38:07.666160 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 66392 of 172148 - 38.57% done - 9:39:36.257671 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 66564 of 172148 - 38.67% done - 9:41:04.438758 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 66736 of 172148 - 38.77% done - 9:42:32.632789 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 66908 of 172148 - 38.87% done - 9:44:01.119765 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 67080 of 172148 - 38.97% done - 9:45:29.577474 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 67252 of 172148 - 39.07% done - 9:46:57.948649 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 67424 of 172148 - 39.17% done - 9:48:26.002928 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 67596 of 172148 - 39.27% done - 9:49:54.552254 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 67768 of 172148 - 39.37% done - 9:51:22.763035 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 67940 of 172148 - 39.47% done - 9:52:51.018580 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 68112 of 172148 - 39.57% done - 9:54:19.508375 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 68284 of 172148 - 39.67% done - 9:55:47.855857 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 68456 of 172148 - 39.77% done - 9:57:16.118567 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 68628 of 172148 - 39.87% done - 9:58:44.825125 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 68800 of 172148 - 39.97% done - 10:00:13.364757 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 68972 of 172148 - 40.07% done - 10:01:42.261731 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 69144 of 172148 - 40.17% done - 10:03:10.530784 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 69316 of 172148 - 40.27% done - 10:04:39.463646 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 69488 of 172148 - 40.37% done - 10:06:07.833152 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 69660 of 172148 - 40.47% done - 10:07:36.221101 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 69832 of 172148 - 40.57% done - 10:09:04.886150 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 70004 of 172148 - 40.67% done - 10:10:33.514990 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 70176 of 172148 - 40.76% done - 10:12:02.925402 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 70348 of 172148 - 40.86% done - 10:13:32.363767 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 70520 of 172148 - 40.96% done - 10:15:01.264635 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 70692 of 172148 - 41.06% done - 10:16:29.837302 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 70864 of 172148 - 41.16% done - 10:17:58.616511 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 71036 of 172148 - 41.26% done - 10:19:27.900079 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 71208 of 172148 - 41.36% done - 10:20:57.785687 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 71380 of 172148 - 41.46% done - 10:22:27.351023 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 71552 of 172148 - 41.56% done - 10:23:56.730163 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 71724 of 172148 - 41.66% done - 10:25:25.905545 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 71896 of 172148 - 41.76% done - 10:26:54.812711 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 72068 of 172148 - 41.86% done - 10:28:23.344444 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 72240 of 172148 - 41.96% done - 10:29:52.574269 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 72412 of 172148 - 42.06% done - 10:31:21.326270 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 72584 of 172148 - 42.16% done - 10:32:49.415740 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 72756 of 172148 - 42.26% done - 10:34:18.889507 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 72928 of 172148 - 42.36% done - 10:35:47.564288 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 73100 of 172148 - 42.46% done - 10:37:15.461545 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 73272 of 172148 - 42.56% done - 10:38:44.231058 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 73444 of 172148 - 42.66% done - 10:40:12.754944 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 73616 of 172148 - 42.76% done - 10:41:40.951162 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 73788 of 172148 - 42.86% done - 10:43:09.335949 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 73960 of 172148 - 42.96% done - 10:44:37.571984 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 74132 of 172148 - 43.06% done - 10:46:05.973185 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 74304 of 172148 - 43.16% done - 10:47:34.210536 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 74476 of 172148 - 43.26% done - 10:49:02.483289 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 74648 of 172148 - 43.36% done - 10:50:31.922999 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 74820 of 172148 - 43.46% done - 10:52:00.125422 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 74992 of 172148 - 43.56% done - 10:53:28.343375 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 75164 of 172148 - 43.66% done - 10:54:57.273203 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 75336 of 172148 - 43.76% done - 10:56:25.399643 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 75508 of 172148 - 43.86% done - 10:57:54.070450 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 75680 of 172148 - 43.96% done - 10:59:22.602498 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 75852 of 172148 - 44.06% done - 11:00:51.317995 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 76024 of 172148 - 44.16% done - 11:02:19.535692 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 76196 of 172148 - 44.26% done - 11:03:48.283308 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 76368 of 172148 - 44.36% done - 11:05:16.558509 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 76540 of 172148 - 44.46% done - 11:06:44.740738 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 76712 of 172148 - 44.56% done - 11:08:13.038371 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 76884 of 172148 - 44.66% done - 11:09:41.538444 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 77056 of 172148 - 44.76% done - 11:11:09.835763 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 77228 of 172148 - 44.86% done - 11:12:37.743667 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 77400 of 172148 - 44.96% done - 11:14:06.168563 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 77572 of 172148 - 45.06% done - 11:15:34.484782 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 77744 of 172148 - 45.16% done - 11:17:02.626239 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 77916 of 172148 - 45.26% done - 11:18:30.701712 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 78088 of 172148 - 45.36% done - 11:20:00.484257 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 78260 of 172148 - 45.46% done - 11:21:28.650722 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 78432 of 172148 - 45.56% done - 11:22:57.575029 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 78604 of 172148 - 45.66% done - 11:24:26.659502 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 78776 of 172148 - 45.76% done - 11:25:54.302496 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 78948 of 172148 - 45.86% done - 11:27:22.114673 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 79120 of 172148 - 45.96% done - 11:28:50.378392 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 79292 of 172148 - 46.06% done - 11:30:18.786291 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 79464 of 172148 - 46.16% done - 11:31:47.197749 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 79636 of 172148 - 46.26% done - 11:33:15.693279 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 79808 of 172148 - 46.36% done - 11:34:44.492431 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 79980 of 172148 - 46.46% done - 11:36:13.241281 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 80152 of 172148 - 46.56% done - 11:37:42.184614 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 80324 of 172148 - 46.66% done - 11:39:10.906933 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 80496 of 172148 - 46.76% done - 11:40:40.405294 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 80668 of 172148 - 46.86% done - 11:42:09.386628 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 80840 of 172148 - 46.96% done - 11:43:37.596259 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 81012 of 172148 - 47.06% done - 11:45:06.413237 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 81184 of 172148 - 47.16% done - 11:46:35.259801 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 81356 of 172148 - 47.26% done - 11:48:04.278610 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 81528 of 172148 - 47.36% done - 11:49:33.553987 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 81700 of 172148 - 47.46% done - 11:51:02.388385 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 81872 of 172148 - 47.56% done - 11:52:31.256097 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 82044 of 172148 - 47.66% done - 11:54:00.219710 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 82216 of 172148 - 47.76% done - 11:55:29.117883 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 82388 of 172148 - 47.86% done - 11:56:58.344118 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 82560 of 172148 - 47.96% done - 11:58:27.294500 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 82732 of 172148 - 48.06% done - 11:59:56.756333 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 82904 of 172148 - 48.16% done - 12:01:26.202547 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 83076 of 172148 - 48.26% done - 12:02:55.297060 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 83248 of 172148 - 48.36% done - 12:04:23.755281 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 83420 of 172148 - 48.46% done - 12:05:53.194595 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 83592 of 172148 - 48.56% done - 12:07:21.670384 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 83764 of 172148 - 48.66% done - 12:08:50.085814 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 83936 of 172148 - 48.76% done - 12:10:19.015480 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 84108 of 172148 - 48.86% done - 12:11:47.453643 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 84280 of 172148 - 48.96% done - 12:13:15.916536 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 84452 of 172148 - 49.06% done - 12:14:45.232503 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 84624 of 172148 - 49.16% done - 12:16:14.141918 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 84796 of 172148 - 49.26% done - 12:17:43.350022 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 84968 of 172148 - 49.36% done - 12:19:13.710162 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 85140 of 172148 - 49.46% done - 12:20:42.618605 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 85312 of 172148 - 49.56% done - 12:22:11.212958 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 85484 of 172148 - 49.66% done - 12:23:40.640925 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 85656 of 172148 - 49.76% done - 12:25:09.789001 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 85828 of 172148 - 49.86% done - 12:26:38.047680 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 86000 of 172148 - 49.96% done - 12:28:06.486466 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 86172 of 172148 - 50.06% done - 12:29:35.162680 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 86344 of 172148 - 50.16% done - 12:31:03.568599 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 86516 of 172148 - 50.26% done - 12:32:32.099611 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 86688 of 172148 - 50.36% done - 12:34:00.609581 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 86860 of 172148 - 50.46% done - 12:35:28.883190 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 87032 of 172148 - 50.56% done - 12:36:57.088515 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 87204 of 172148 - 50.66% done - 12:38:25.612142 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 87376 of 172148 - 50.76% done - 12:39:54.143644 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 87548 of 172148 - 50.86% done - 12:41:22.522240 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 87720 of 172148 - 50.96% done - 12:42:50.919275 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 87892 of 172148 - 51.06% done - 12:44:19.605569 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 88064 of 172148 - 51.16% done - 12:45:47.615043 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 88236 of 172148 - 51.26% done - 12:47:16.016748 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 88408 of 172148 - 51.36% done - 12:48:45.046279 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 88580 of 172148 - 51.46% done - 12:50:13.320936 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 88752 of 172148 - 51.56% done - 12:51:42.037098 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 88924 of 172148 - 51.66% done - 12:53:10.254749 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 89096 of 172148 - 51.76% done - 12:54:38.722554 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 89268 of 172148 - 51.86% done - 12:56:07.061350 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 89440 of 172148 - 51.96% done - 12:57:35.168140 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 89612 of 172148 - 52.06% done - 12:59:03.510084 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 89784 of 172148 - 52.16% done - 13:00:31.702717 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 89956 of 172148 - 52.26% done - 13:02:00.485143 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 90128 of 172148 - 52.35% done - 13:03:29.580799 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 90300 of 172148 - 52.45% done - 13:04:58.091715 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 90472 of 172148 - 52.55% done - 13:06:25.973609 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 90644 of 172148 - 52.65% done - 13:07:54.062772 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 90816 of 172148 - 52.75% done - 13:09:24.729509 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 90988 of 172148 - 52.85% done - 13:10:56.764641 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 91160 of 172148 - 52.95% done - 13:12:29.646586 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 91332 of 172148 - 53.05% done - 13:14:02.342562 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 91504 of 172148 - 53.15% done - 13:15:35.812483 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 91676 of 172148 - 53.25% done - 13:17:09.297775 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 91848 of 172148 - 53.35% done - 13:18:43.094440 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 92020 of 172148 - 53.45% done - 13:20:12.845976 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 92192 of 172148 - 53.55% done - 13:21:41.642311 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 92364 of 172148 - 53.65% done - 13:23:10.909269 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 92536 of 172148 - 53.75% done - 13:24:40.352188 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 92708 of 172148 - 53.85% done - 13:26:09.455268 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 92880 of 172148 - 53.95% done - 13:27:38.331408 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 93052 of 172148 - 54.05% done - 13:29:07.329003 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 93224 of 172148 - 54.15% done - 13:30:36.629908 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 93396 of 172148 - 54.25% done - 13:32:05.697461 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 93568 of 172148 - 54.35% done - 13:33:37.125204 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 93740 of 172148 - 54.45% done - 13:35:06.819045 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 93912 of 172148 - 54.55% done - 13:36:36.287368 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 94084 of 172148 - 54.65% done - 13:38:05.502006 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 94256 of 172148 - 54.75% done - 13:39:35.300045 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 94428 of 172148 - 54.85% done - 13:41:04.685589 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 94600 of 172148 - 54.95% done - 13:42:34.523966 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 94772 of 172148 - 55.05% done - 13:44:03.967786 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 94944 of 172148 - 55.15% done - 13:45:33.139396 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 95116 of 172148 - 55.25% done - 13:47:02.328162 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 95288 of 172148 - 55.35% done - 13:48:31.562462 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 95460 of 172148 - 55.45% done - 13:50:01.834132 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 95632 of 172148 - 55.55% done - 13:51:32.025856 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 95804 of 172148 - 55.65% done - 13:53:01.055428 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 95976 of 172148 - 55.75% done - 13:54:29.870388 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 96148 of 172148 - 55.85% done - 13:55:58.477709 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 96320 of 172148 - 55.95% done - 13:57:26.980897 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 96492 of 172148 - 56.05% done - 13:58:55.776816 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 96664 of 172148 - 56.15% done - 14:00:24.246261 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 96836 of 172148 - 56.25% done - 14:01:53.144295 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 97008 of 172148 - 56.35% done - 14:03:21.622549 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 97180 of 172148 - 56.45% done - 14:04:50.142677 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 97352 of 172148 - 56.55% done - 14:06:18.398140 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 97524 of 172148 - 56.65% done - 14:07:46.820760 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 97696 of 172148 - 56.75% done - 14:09:15.336865 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 97868 of 172148 - 56.85% done - 14:10:43.725159 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 98040 of 172148 - 56.95% done - 14:12:12.022174 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 98212 of 172148 - 57.05% done - 14:13:40.425433 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 98384 of 172148 - 57.15% done - 14:15:08.500686 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 98556 of 172148 - 57.25% done - 14:16:36.720330 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 98728 of 172148 - 57.35% done - 14:18:05.035248 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 98900 of 172148 - 57.45% done - 14:19:34.757027 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 99072 of 172148 - 57.55% done - 14:21:03.890889 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 99244 of 172148 - 57.65% done - 14:22:32.857827 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 99416 of 172148 - 57.75% done - 14:24:01.917620 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 99588 of 172148 - 57.85% done - 14:25:30.144067 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 99760 of 172148 - 57.95% done - 14:26:58.900251 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 99932 of 172148 - 58.05% done - 14:28:27.466591 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 100104 of 172148 - 58.15% done - 14:29:55.802499 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 100276 of 172148 - 58.25% done - 14:31:24.244854 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 100448 of 172148 - 58.35% done - 14:32:52.888247 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 100620 of 172148 - 58.45% done - 14:34:21.251372 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 100792 of 172148 - 58.55% done - 14:35:49.733538 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 100964 of 172148 - 58.65% done - 14:37:17.847903 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 101136 of 172148 - 58.75% done - 14:38:46.578115 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 101308 of 172148 - 58.85% done - 14:40:14.918751 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 101480 of 172148 - 58.95% done - 14:41:42.874293 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 101652 of 172148 - 59.05% done - 14:43:11.151400 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 101824 of 172148 - 59.15% done - 14:44:39.526790 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 101996 of 172148 - 59.25% done - 14:46:07.787667 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 102168 of 172148 - 59.35% done - 14:47:36.111292 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 102340 of 172148 - 59.45% done - 14:49:05.263956 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 102512 of 172148 - 59.55% done - 14:50:33.446880 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 102684 of 172148 - 59.65% done - 14:52:01.984172 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 102856 of 172148 - 59.75% done - 14:53:30.172353 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 103028 of 172148 - 59.85% done - 14:54:59.398717 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 103200 of 172148 - 59.95% done - 14:56:27.770257 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 103372 of 172148 - 60.05% done - 14:57:56.103172 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 103544 of 172148 - 60.15% done - 14:59:24.263667 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 103716 of 172148 - 60.25% done - 15:00:52.801670 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 103888 of 172148 - 60.35% done - 15:02:21.522445 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 104060 of 172148 - 60.45% done - 15:03:50.867880 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 104232 of 172148 - 60.55% done - 15:05:19.049055 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 104404 of 172148 - 60.65% done - 15:06:48.899371 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 104576 of 172148 - 60.75% done - 15:08:17.448556 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 104748 of 172148 - 60.85% done - 15:09:46.037644 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 104920 of 172148 - 60.95% done - 15:11:16.182534 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 105092 of 172148 - 61.05% done - 15:12:45.528457 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 105264 of 172148 - 61.15% done - 15:14:18.019943 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 105436 of 172148 - 61.25% done - 15:15:48.114558 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 105608 of 172148 - 61.35% done - 15:17:16.214051 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 105780 of 172148 - 61.45% done - 15:18:45.373787 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 105952 of 172148 - 61.55% done - 15:20:14.888681 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 106124 of 172148 - 61.65% done - 15:21:43.875619 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 106296 of 172148 - 61.75% done - 15:23:12.764685 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 106468 of 172148 - 61.85% done - 15:24:41.815402 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 106640 of 172148 - 61.95% done - 15:26:11.898370 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 106812 of 172148 - 62.05% done - 15:27:41.793444 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 106984 of 172148 - 62.15% done - 15:29:11.887064 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 107156 of 172148 - 62.25% done - 15:30:42.308003 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 107328 of 172148 - 62.35% done - 15:32:12.930920 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 107500 of 172148 - 62.45% done - 15:33:43.356625 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 107672 of 172148 - 62.55% done - 15:35:14.151263 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 107844 of 172148 - 62.65% done - 15:36:48.286031 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 108016 of 172148 - 62.75% done - 15:38:23.932188 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 108188 of 172148 - 62.85% done - 15:40:07.230184 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 108360 of 172148 - 62.95% done - 15:41:52.556174 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 108532 of 172148 - 63.05% done - 15:43:37.755243 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 108704 of 172148 - 63.15% done - 15:45:22.010843 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 108876 of 172148 - 63.25% done - 15:47:05.042663 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 109048 of 172148 - 63.35% done - 15:48:46.505357 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 109220 of 172148 - 63.45% done - 15:50:26.084765 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 109392 of 172148 - 63.55% done - 15:51:57.030065 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 109564 of 172148 - 63.65% done - 15:53:28.136691 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 109736 of 172148 - 63.75% done - 15:55:00.607533 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 109908 of 172148 - 63.85% done - 15:56:32.968944 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 110080 of 172148 - 63.94% done - 15:58:04.931682 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 110252 of 172148 - 64.04% done - 15:59:37.353486 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 110424 of 172148 - 64.14% done - 16:01:10.058370 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 110596 of 172148 - 64.24% done - 16:02:41.932626 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 110768 of 172148 - 64.34% done - 16:04:11.012988 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 110940 of 172148 - 64.44% done - 16:05:39.279738 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 111112 of 172148 - 64.54% done - 16:07:07.270718 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 111284 of 172148 - 64.64% done - 16:08:35.839648 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 111456 of 172148 - 64.74% done - 16:10:04.167654 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 111628 of 172148 - 64.84% done - 16:11:32.078405 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 111800 of 172148 - 64.94% done - 16:13:00.233179 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 111972 of 172148 - 65.04% done - 16:14:28.450100 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 112144 of 172148 - 65.14% done - 16:15:56.525842 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 112316 of 172148 - 65.24% done - 16:17:25.711250 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 112488 of 172148 - 65.34% done - 16:19:06.840460 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 112660 of 172148 - 65.44% done - 16:20:55.220925 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 112832 of 172148 - 65.54% done - 16:22:42.857932 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 113004 of 172148 - 65.64% done - 16:24:24.644434 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 113176 of 172148 - 65.74% done - 16:25:58.828010 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 113348 of 172148 - 65.84% done - 16:27:31.784109 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 113520 of 172148 - 65.94% done - 16:29:05.523718 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 113692 of 172148 - 66.04% done - 16:30:39.183454 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 113864 of 172148 - 66.14% done - 16:32:12.787659 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 114036 of 172148 - 66.24% done - 16:33:46.691893 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 114208 of 172148 - 66.34% done - 16:35:21.158683 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 114380 of 172148 - 66.44% done - 16:36:54.954502 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 114552 of 172148 - 66.54% done - 16:38:28.318257 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 114724 of 172148 - 66.64% done - 16:40:02.072691 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 114896 of 172148 - 66.74% done - 16:41:35.604798 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 115068 of 172148 - 66.84% done - 16:43:10.333669 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 115240 of 172148 - 66.94% done - 16:44:44.093934 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 115412 of 172148 - 67.04% done - 16:46:17.446877 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 115584 of 172148 - 67.14% done - 16:47:51.106521 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 115756 of 172148 - 67.24% done - 16:49:24.802717 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 115928 of 172148 - 67.34% done - 16:50:58.572955 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 116100 of 172148 - 67.44% done - 16:52:38.232276 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 116272 of 172148 - 67.54% done - 16:54:11.449142 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 116444 of 172148 - 67.64% done - 16:55:43.772691 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 116616 of 172148 - 67.74% done - 16:57:16.959526 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 116788 of 172148 - 67.84% done - 16:58:49.968705 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 116960 of 172148 - 67.94% done - 17:00:23.976779 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 117132 of 172148 - 68.04% done - 17:01:58.997507 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 117304 of 172148 - 68.14% done - 17:03:32.099106 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 117476 of 172148 - 68.24% done - 17:05:03.373057 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 117648 of 172148 - 68.34% done - 17:06:31.485825 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 117820 of 172148 - 68.44% done - 17:07:59.913056 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 117992 of 172148 - 68.54% done - 17:09:28.786541 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 118164 of 172148 - 68.64% done - 17:10:57.319104 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 118336 of 172148 - 68.74% done - 17:12:25.267958 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 118508 of 172148 - 68.84% done - 17:13:54.345863 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 118680 of 172148 - 68.94% done - 17:15:22.998676 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 118852 of 172148 - 69.04% done - 17:16:51.786865 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 119024 of 172148 - 69.14% done - 17:18:20.782787 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 119196 of 172148 - 69.24% done - 17:19:53.675113 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 119368 of 172148 - 69.34% done - 17:21:28.392046 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 119540 of 172148 - 69.44% done - 17:23:02.477505 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 119712 of 172148 - 69.54% done - 17:24:36.053265 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 119884 of 172148 - 69.64% done - 17:26:09.820169 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 120056 of 172148 - 69.74% done - 17:27:43.184691 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 120228 of 172148 - 69.84% done - 17:29:16.497196 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 120400 of 172148 - 69.94% done - 17:30:47.575477 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 120572 of 172148 - 70.04% done - 17:32:15.851514 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 120744 of 172148 - 70.14% done - 17:33:46.240842 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 120916 of 172148 - 70.24% done - 17:35:15.773182 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 121088 of 172148 - 70.34% done - 17:36:44.645169 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 121260 of 172148 - 70.44% done - 17:38:13.448478 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 121432 of 172148 - 70.54% done - 17:39:42.547838 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 121604 of 172148 - 70.64% done - 17:41:11.207368 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 121776 of 172148 - 70.74% done - 17:42:40.030058 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 121948 of 172148 - 70.84% done - 17:44:08.969985 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 122120 of 172148 - 70.94% done - 17:45:37.320664 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 122292 of 172148 - 71.04% done - 17:47:06.075052 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 122464 of 172148 - 71.14% done - 17:48:34.894443 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 122636 of 172148 - 71.24% done - 17:50:04.898855 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 122808 of 172148 - 71.34% done - 17:51:34.059006 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 122980 of 172148 - 71.44% done - 17:53:03.115318 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 123152 of 172148 - 71.54% done - 17:54:32.377025 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 123324 of 172148 - 71.64% done - 17:56:01.604070 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 123496 of 172148 - 71.74% done - 17:57:30.657881 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 123668 of 172148 - 71.84% done - 17:59:00.291879 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 123840 of 172148 - 71.94% done - 18:00:29.330597 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 124012 of 172148 - 72.04% done - 18:01:58.764738 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 124184 of 172148 - 72.14% done - 18:03:27.131131 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 124356 of 172148 - 72.24% done - 18:04:55.833072 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 124528 of 172148 - 72.34% done - 18:06:25.971298 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 124700 of 172148 - 72.44% done - 18:07:54.569385 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 124872 of 172148 - 72.54% done - 18:09:23.602036 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 125044 of 172148 - 72.64% done - 18:10:52.612954 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 125216 of 172148 - 72.74% done - 18:12:21.367461 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 125388 of 172148 - 72.84% done - 18:13:50.727357 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 125560 of 172148 - 72.94% done - 18:15:19.692408 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 125732 of 172148 - 73.04% done - 18:16:49.250039 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 125904 of 172148 - 73.14% done - 18:18:18.272386 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 126076 of 172148 - 73.24% done - 18:19:48.773636 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 126248 of 172148 - 73.34% done - 18:21:18.474811 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 126420 of 172148 - 73.44% done - 18:22:48.151811 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 126592 of 172148 - 73.54% done - 18:24:17.957649 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 126764 of 172148 - 73.64% done - 18:25:47.388929 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 126936 of 172148 - 73.74% done - 18:27:16.876328 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 127108 of 172148 - 73.84% done - 18:28:46.614515 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 127280 of 172148 - 73.94% done - 18:30:16.462242 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 127452 of 172148 - 74.04% done - 18:31:46.024417 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 127624 of 172148 - 74.14% done - 18:33:15.275414 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 127796 of 172148 - 74.24% done - 18:34:44.514254 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 127968 of 172148 - 74.34% done - 18:36:13.687722 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 128140 of 172148 - 74.44% done - 18:37:42.354520 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 128312 of 172148 - 74.54% done - 18:39:11.123061 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 128484 of 172148 - 74.64% done - 18:40:40.332257 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 128656 of 172148 - 74.74% done - 18:42:08.775061 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 128828 of 172148 - 74.84% done - 18:43:37.521039 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 129000 of 172148 - 74.94% done - 18:45:06.159819 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 129172 of 172148 - 75.04% done - 18:46:34.663716 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 129344 of 172148 - 75.14% done - 18:48:03.361538 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 129516 of 172148 - 75.24% done - 18:49:32.831426 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 129688 of 172148 - 75.34% done - 18:51:01.741825 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 129860 of 172148 - 75.44% done - 18:52:30.652926 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 130032 of 172148 - 75.54% done - 18:53:59.858258 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 130204 of 172148 - 75.63% done - 18:55:29.001048 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 130376 of 172148 - 75.73% done - 18:56:57.744851 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 130548 of 172148 - 75.83% done - 18:58:26.673493 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 130720 of 172148 - 75.93% done - 18:59:55.838431 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 130892 of 172148 - 76.03% done - 19:01:25.490353 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 131064 of 172148 - 76.13% done - 19:02:54.658644 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 131236 of 172148 - 76.23% done - 19:04:24.514477 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 131408 of 172148 - 76.33% done - 19:05:53.208714 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 131580 of 172148 - 76.43% done - 19:07:21.768334 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 131752 of 172148 - 76.53% done - 19:08:50.440941 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 131924 of 172148 - 76.63% done - 19:10:19.048441 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 132096 of 172148 - 76.73% done - 19:11:47.651048 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 132268 of 172148 - 76.83% done - 19:13:16.156980 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 132440 of 172148 - 76.93% done - 19:14:45.462075 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 132612 of 172148 - 77.03% done - 19:16:14.110480 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 132784 of 172148 - 77.13% done - 19:17:42.862285 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 132956 of 172148 - 77.23% done - 19:19:11.636881 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 133128 of 172148 - 77.33% done - 19:20:40.699836 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 133300 of 172148 - 77.43% done - 19:22:09.025355 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 133472 of 172148 - 77.53% done - 19:23:37.687794 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 133644 of 172148 - 77.63% done - 19:25:06.466063 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 133816 of 172148 - 77.73% done - 19:26:34.793408 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 133988 of 172148 - 77.83% done - 19:28:03.572693 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 134160 of 172148 - 77.93% done - 19:29:32.333056 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 134332 of 172148 - 78.03% done - 19:31:02.330699 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 134504 of 172148 - 78.13% done - 19:32:31.350199 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 134676 of 172148 - 78.23% done - 19:34:00.483640 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 134848 of 172148 - 78.33% done - 19:35:29.102089 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 135020 of 172148 - 78.43% done - 19:36:58.544925 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 135192 of 172148 - 78.53% done - 19:38:27.203179 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 135364 of 172148 - 78.63% done - 19:39:56.603056 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 135536 of 172148 - 78.73% done - 19:41:25.141493 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 135708 of 172148 - 78.83% done - 19:42:54.029663 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 135880 of 172148 - 78.93% done - 19:44:22.739694 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 136052 of 172148 - 79.03% done - 19:45:51.399860 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 136224 of 172148 - 79.13% done - 19:47:19.569573 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 136396 of 172148 - 79.23% done - 19:48:49.409174 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 136568 of 172148 - 79.33% done - 19:50:18.222261 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 136740 of 172148 - 79.43% done - 19:51:46.991139 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 136912 of 172148 - 79.53% done - 19:53:15.656124 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 137084 of 172148 - 79.63% done - 19:54:44.621363 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 137256 of 172148 - 79.73% done - 19:56:13.185957 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 137428 of 172148 - 79.83% done - 19:57:41.982933 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 137600 of 172148 - 79.93% done - 19:59:10.960242 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 137772 of 172148 - 80.03% done - 20:00:39.868232 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 137944 of 172148 - 80.13% done - 20:02:08.962587 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 138116 of 172148 - 80.23% done - 20:03:37.851420 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 138288 of 172148 - 80.33% done - 20:05:06.617830 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 138460 of 172148 - 80.43% done - 20:06:35.321446 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 138632 of 172148 - 80.53% done - 20:08:03.767828 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 138804 of 172148 - 80.63% done - 20:09:32.197444 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 138976 of 172148 - 80.73% done - 20:11:00.616083 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 139148 of 172148 - 80.83% done - 20:12:28.831816 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 139320 of 172148 - 80.93% done - 20:13:57.538224 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 139492 of 172148 - 81.03% done - 20:15:25.860517 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 139664 of 172148 - 81.13% done - 20:16:54.239503 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 139836 of 172148 - 81.23% done - 20:18:22.874916 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 140008 of 172148 - 81.33% done - 20:19:53.683067 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 140180 of 172148 - 81.43% done - 20:21:22.817017 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 140352 of 172148 - 81.53% done - 20:22:50.761669 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 140524 of 172148 - 81.63% done - 20:24:18.759862 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 140696 of 172148 - 81.73% done - 20:25:46.707151 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 140868 of 172148 - 81.83% done - 20:27:14.876428 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 141040 of 172148 - 81.93% done - 20:28:43.324437 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 141212 of 172148 - 82.03% done - 20:30:12.227540 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 141384 of 172148 - 82.13% done - 20:31:41.429756 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 141556 of 172148 - 82.23% done - 20:33:09.807983 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 141728 of 172148 - 82.33% done - 20:34:38.631147 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 141900 of 172148 - 82.43% done - 20:36:07.018362 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 142072 of 172148 - 82.53% done - 20:37:35.682630 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 142244 of 172148 - 82.63% done - 20:39:04.034041 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 142416 of 172148 - 82.73% done - 20:40:32.676360 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 142588 of 172148 - 82.83% done - 20:42:01.502929 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 142760 of 172148 - 82.93% done - 20:43:30.123952 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 142932 of 172148 - 83.03% done - 20:44:59.052260 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 143104 of 172148 - 83.13% done - 20:46:27.534630 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 143276 of 172148 - 83.23% done - 20:47:55.805893 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 143448 of 172148 - 83.33% done - 20:49:24.388318 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 143620 of 172148 - 83.43% done - 20:50:53.141484 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 143792 of 172148 - 83.53% done - 20:52:21.511000 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 143964 of 172148 - 83.63% done - 20:53:50.469865 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 144136 of 172148 - 83.73% done - 20:55:18.793262 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 144308 of 172148 - 83.83% done - 20:56:47.302286 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 144480 of 172148 - 83.93% done - 20:58:15.805039 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 144652 of 172148 - 84.03% done - 20:59:44.798466 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 144824 of 172148 - 84.13% done - 21:01:13.861609 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 144996 of 172148 - 84.23% done - 21:02:42.675267 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 145168 of 172148 - 84.33% done - 21:04:12.318300 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 145340 of 172148 - 84.43% done - 21:05:40.728015 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 145512 of 172148 - 84.53% done - 21:07:09.045150 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 145684 of 172148 - 84.63% done - 21:08:37.906753 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 145856 of 172148 - 84.73% done - 21:10:06.677320 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 146028 of 172148 - 84.83% done - 21:11:34.853405 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 146200 of 172148 - 84.93% done - 21:13:03.156767 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 146372 of 172148 - 85.03% done - 21:14:31.877756 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 146544 of 172148 - 85.13% done - 21:16:00.493257 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 146716 of 172148 - 85.23% done - 21:17:28.753074 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 146888 of 172148 - 85.33% done - 21:18:57.459156 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 147060 of 172148 - 85.43% done - 21:20:26.217508 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 147232 of 172148 - 85.53% done - 21:21:54.538486 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 147404 of 172148 - 85.63% done - 21:23:22.668315 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 147576 of 172148 - 85.73% done - 21:24:51.852071 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 147748 of 172148 - 85.83% done - 21:26:20.081477 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 147920 of 172148 - 85.93% done - 21:27:48.433248 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 148092 of 172148 - 86.03% done - 21:29:16.329216 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 148264 of 172148 - 86.13% done - 21:30:44.670334 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 148436 of 172148 - 86.23% done - 21:32:13.110964 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 148608 of 172148 - 86.33% done - 21:33:41.514899 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 148780 of 172148 - 86.43% done - 21:35:10.049524 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 148952 of 172148 - 86.53% done - 21:36:38.128933 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 149124 of 172148 - 86.63% done - 21:38:06.089412 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 149296 of 172148 - 86.73% done - 21:39:34.533794 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 149468 of 172148 - 86.83% done - 21:41:02.856877 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 149640 of 172148 - 86.93% done - 21:42:31.129854 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 149812 of 172148 - 87.03% done - 21:43:59.691755 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 149984 of 172148 - 87.13% done - 21:45:28.030432 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 150156 of 172148 - 87.22% done - 21:46:56.466740 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 150328 of 172148 - 87.32% done - 21:48:24.653338 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 150500 of 172148 - 87.42% done - 21:49:53.861258 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 150672 of 172148 - 87.52% done - 21:51:22.554106 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 150844 of 172148 - 87.62% done - 21:52:50.790055 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 151016 of 172148 - 87.72% done - 21:54:19.128352 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 151188 of 172148 - 87.82% done - 21:55:47.508946 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 151360 of 172148 - 87.92% done - 21:57:16.004625 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 151532 of 172148 - 88.02% done - 21:58:44.586692 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 151704 of 172148 - 88.12% done - 22:00:13.179683 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 151876 of 172148 - 88.22% done - 22:01:41.625804 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 152048 of 172148 - 88.32% done - 22:03:09.971322 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 152220 of 172148 - 88.42% done - 22:04:38.425232 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 152392 of 172148 - 88.52% done - 22:06:06.463132 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 152564 of 172148 - 88.62% done - 22:07:34.735344 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 152736 of 172148 - 88.72% done - 22:09:03.159913 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 152908 of 172148 - 88.82% done - 22:10:31.316566 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 153080 of 172148 - 88.92% done - 22:11:59.806001 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 153252 of 172148 - 89.02% done - 22:13:28.043239 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 153424 of 172148 - 89.12% done - 22:14:56.753020 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 153596 of 172148 - 89.22% done - 22:16:25.182837 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 153768 of 172148 - 89.32% done - 22:17:53.106211 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 153940 of 172148 - 89.42% done - 22:19:21.772323 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 154112 of 172148 - 89.52% done - 22:20:51.166720 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 154284 of 172148 - 89.62% done - 22:22:20.331427 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 154456 of 172148 - 89.72% done - 22:23:48.981833 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 154628 of 172148 - 89.82% done - 22:25:17.098192 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 154800 of 172148 - 89.92% done - 22:26:45.779162 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 154972 of 172148 - 90.02% done - 22:28:14.338320 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 155144 of 172148 - 90.12% done - 22:29:43.037322 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 155316 of 172148 - 90.22% done - 22:31:11.538142 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 155488 of 172148 - 90.32% done - 22:32:40.094850 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 155660 of 172148 - 90.42% done - 22:34:08.367140 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 155832 of 172148 - 90.52% done - 22:35:36.364977 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 156004 of 172148 - 90.62% done - 22:37:04.839542 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 156176 of 172148 - 90.72% done - 22:38:33.058171 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 156348 of 172148 - 90.82% done - 22:40:01.619160 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 156520 of 172148 - 90.92% done - 22:41:29.977008 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 156692 of 172148 - 91.02% done - 22:42:58.471274 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 156864 of 172148 - 91.12% done - 22:44:26.900019 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 157036 of 172148 - 91.22% done - 22:45:55.247399 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 157208 of 172148 - 91.32% done - 22:47:23.771054 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 157380 of 172148 - 91.42% done - 22:48:52.096743 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 157552 of 172148 - 91.52% done - 22:50:20.980799 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 157724 of 172148 - 91.62% done - 22:51:49.586358 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 157896 of 172148 - 91.72% done - 22:53:17.883071 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 158068 of 172148 - 91.82% done - 22:54:46.408888 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 158240 of 172148 - 91.92% done - 22:56:14.480195 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 158412 of 172148 - 92.02% done - 22:57:42.468542 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 158584 of 172148 - 92.12% done - 22:59:10.902493 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 158756 of 172148 - 92.22% done - 23:00:39.522375 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 158928 of 172148 - 92.32% done - 23:02:07.910163 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 159100 of 172148 - 92.42% done - 23:03:36.141239 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 159272 of 172148 - 92.52% done - 23:05:04.590175 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 159444 of 172148 - 92.62% done - 23:06:32.877630 ellapsed, est. total time: 25.0 hr\n",
      "[epoch=0] batch 159616 of 172148 - 92.72% done - 23:08:01.250713 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 159788 of 172148 - 92.82% done - 23:09:29.603016 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 159960 of 172148 - 92.92% done - 23:10:57.894241 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 160132 of 172148 - 93.02% done - 23:12:25.894533 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 160304 of 172148 - 93.12% done - 23:13:54.288069 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 160476 of 172148 - 93.22% done - 23:15:22.437879 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 160648 of 172148 - 93.32% done - 23:16:50.475545 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 160820 of 172148 - 93.42% done - 23:18:18.378656 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 160992 of 172148 - 93.52% done - 23:19:47.297988 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 161164 of 172148 - 93.62% done - 23:21:15.476362 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 161336 of 172148 - 93.72% done - 23:22:43.542489 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 161508 of 172148 - 93.82% done - 23:24:12.268583 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 161680 of 172148 - 93.92% done - 23:25:40.820498 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 161852 of 172148 - 94.02% done - 23:27:09.112555 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 162024 of 172148 - 94.12% done - 23:28:37.466013 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 162196 of 172148 - 94.22% done - 23:30:05.885296 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 162368 of 172148 - 94.32% done - 23:31:34.009483 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 162540 of 172148 - 94.42% done - 23:33:02.165610 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 162712 of 172148 - 94.52% done - 23:34:30.918859 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 162884 of 172148 - 94.62% done - 23:35:59.223641 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 163056 of 172148 - 94.72% done - 23:37:27.493002 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 163228 of 172148 - 94.82% done - 23:38:56.152329 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 163400 of 172148 - 94.92% done - 23:40:24.454876 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 163572 of 172148 - 95.02% done - 23:41:52.824731 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 163744 of 172148 - 95.12% done - 23:43:21.053177 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 163916 of 172148 - 95.22% done - 23:44:49.481417 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 164088 of 172148 - 95.32% done - 23:46:17.411385 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 164260 of 172148 - 95.42% done - 23:47:45.656580 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 164432 of 172148 - 95.52% done - 23:49:14.194755 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 164604 of 172148 - 95.62% done - 23:50:42.946891 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 164776 of 172148 - 95.72% done - 23:52:11.311730 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 164948 of 172148 - 95.82% done - 23:53:39.993509 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 165120 of 172148 - 95.92% done - 23:55:08.028177 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 165292 of 172148 - 96.02% done - 23:56:37.704968 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 165464 of 172148 - 96.12% done - 23:58:07.852296 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 165636 of 172148 - 96.22% done - 23:59:40.402178 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 165808 of 172148 - 96.32% done - 1 day, 0:01:11.187009 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 165980 of 172148 - 96.42% done - 1 day, 0:02:51.727431 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 166152 of 172148 - 96.52% done - 1 day, 0:04:23.212268 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 166324 of 172148 - 96.62% done - 1 day, 0:05:54.292025 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 166496 of 172148 - 96.72% done - 1 day, 0:07:24.735484 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 166668 of 172148 - 96.82% done - 1 day, 0:08:52.937143 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 166840 of 172148 - 96.92% done - 1 day, 0:10:21.309366 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 167012 of 172148 - 97.02% done - 1 day, 0:11:49.680220 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 167184 of 172148 - 97.12% done - 1 day, 0:13:17.922544 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 167356 of 172148 - 97.22% done - 1 day, 0:14:46.352479 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 167528 of 172148 - 97.32% done - 1 day, 0:16:14.640846 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 167700 of 172148 - 97.42% done - 1 day, 0:17:42.988041 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 167872 of 172148 - 97.52% done - 1 day, 0:19:11.360969 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 168044 of 172148 - 97.62% done - 1 day, 0:20:42.008427 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 168216 of 172148 - 97.72% done - 1 day, 0:22:10.725847 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 168388 of 172148 - 97.82% done - 1 day, 0:23:39.526844 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 168560 of 172148 - 97.92% done - 1 day, 0:25:08.135018 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 168732 of 172148 - 98.02% done - 1 day, 0:26:36.771638 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 168904 of 172148 - 98.12% done - 1 day, 0:28:05.679797 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 169076 of 172148 - 98.22% done - 1 day, 0:29:34.307540 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 169248 of 172148 - 98.32% done - 1 day, 0:31:03.046488 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 169420 of 172148 - 98.42% done - 1 day, 0:32:31.530749 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 169592 of 172148 - 98.52% done - 1 day, 0:34:00.481641 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 169764 of 172148 - 98.62% done - 1 day, 0:35:28.812048 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 169936 of 172148 - 98.72% done - 1 day, 0:36:56.964103 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 170108 of 172148 - 98.81% done - 1 day, 0:38:25.365866 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 170280 of 172148 - 98.91% done - 1 day, 0:39:54.478100 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 170452 of 172148 - 99.01% done - 1 day, 0:41:22.659277 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 170624 of 172148 - 99.11% done - 1 day, 0:42:50.997438 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 170796 of 172148 - 99.21% done - 1 day, 0:44:19.283144 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 170968 of 172148 - 99.31% done - 1 day, 0:45:47.899250 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 171140 of 172148 - 99.41% done - 1 day, 0:47:16.276205 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 171312 of 172148 - 99.51% done - 1 day, 0:48:45.327898 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 171484 of 172148 - 99.61% done - 1 day, 0:50:13.903869 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 171656 of 172148 - 99.71% done - 1 day, 0:51:42.480990 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 171828 of 172148 - 99.81% done - 1 day, 0:53:11.335194 ellapsed, est. total time: 24.9 hr\n",
      "[epoch=0] batch 172000 of 172148 - 99.91% done - 1 day, 0:54:40.055844 ellapsed, est. total time: 24.9 hr\n",
      "CPU times: user 3d 12h 48min 7s, sys: 14h 28min 53s, total: 4d 3h 17min 1s\n",
      "Wall time: 1d 55min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_seed(42) # reproducible\n",
    "\n",
    "# - - - - - - - - \n",
    "# build model\n",
    "# - - - - - - - - \n",
    "model = DecoderLanguageModel(\n",
    "    vocab_size, emb_dim, num_heads, num_blocks, pad_idx\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "initialize_weights(model)   \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {total_params}\\n\")\n",
    "\n",
    "# - - - - - - - - \n",
    "# train model\n",
    "# - - - - - - - - \n",
    "start = datetime.now()\n",
    "update_batch_cnt = len(train_dataloader) // 1000  # print an update every 0.1% of progress\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(train_dataloader):\n",
    "\n",
    "        if batch_idx % update_batch_cnt == 0:\n",
    "            ellapsed = datetime.now() - start\n",
    "            percent_done = 100 * batch_idx/len(train_dataloader)\n",
    "            estimated_total_time = 'unknown'\n",
    "            if percent_done > 0:\n",
    "                estimated_total_time = f'{ellapsed.total_seconds() / (percent_done/100) / (60 * 60):.1f} hr'\n",
    "            print(f'[epoch={epoch}] batch {batch_idx} of {len(train_dataloader)} - {percent_done:.2f}% done - {timedelta(seconds=ellapsed.total_seconds())} ellapsed, est. total time: {estimated_total_time}')\n",
    "        \n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "\n",
    "        logits = model(input_seq)\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        target = target_seq.reshape(-1)\n",
    "        mask = target != pad_idx # mask to exclude padding tokens\n",
    "        \n",
    "        loss = criterion(logits[mask], target[mask]) # compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84e30246-510b-4df5-bd5c-ba9f33fcd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_file = \"./transformer.pth\"\n",
    "torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7317b-009c-489d-94e9-78ccfd67e8a6",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d25d62e5-b84d-4aa7-b046-cdb40f19e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [\n",
    "    \"Moscow\",\n",
    "    \"New York\",\n",
    "    \"A hurricane\",\n",
    "    \"The President\",\n",
    "    \"The Washington Nationals\",\n",
    "    \"Canada is known for\"\n",
    "]\n",
    "\n",
    "def evaluate_model(a_model, contexts=contexts):\n",
    "    a_model.eval()\n",
    "    for context in contexts:\n",
    "        generated_text = generate_text(\n",
    "            model=a_model,\n",
    "            start_string=context,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            max_length=50\n",
    "        )\n",
    "        print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "924d113d-d224-43be-a193-8c1a7f34304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moscow has been accused of being a member of . ' '' 'The . ' '' 's statement said . 's statement said . 'The . ' '' 's statement was written . 's . 's 's 's 'not a '\n",
      "New York City 's ##-year-old son , Michael , was found dead in a car park in the town of <rare> , near the city 's capital . 's Central Park . 's Day . 's 's time . '\n",
      "A hurricane has been hit by the storms , the New York Times reported . '### % of the ##-year-old 's ##-year-old , and the ##-thee for ## minutes of the up toions of the way\n",
      "The President of the United States has been in the midst of a war in the past . '' 's presidential election . ' '' 's statement said . 's statement said . 's decision was 's 's 's ' ' ' ' he\n",
      "The Washington Nationals 's first-ever presidential election campaign was announced in #### . '' 's #### manifesto . ' '' 's statement said . 's . 's presidential campaign . 's president . 's . 's . ' of the\n",
      "Canada is known for its most recent years . ' '' 's report said . ' '' 'The ##-year-old was a 'sweet ' . ' ' . ' 's . 's 's 's 's ' . ' 's ' ,\n"
     ]
    }
   ],
   "source": [
    "# test the saved model\n",
    "model2 = DecoderLanguageModel(\n",
    "    vocab_size, emb_dim, num_heads, num_blocks, pad_idx\n",
    ")\n",
    "model2.load_state_dict(torch.load(model_file, weights_only=True))\n",
    "model2.to(device)\n",
    "\n",
    "evaluate_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfb5ec-203d-48e2-aa1b-769193383778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

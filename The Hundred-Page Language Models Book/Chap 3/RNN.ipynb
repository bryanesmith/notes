{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "063a66d2-40b7-4670-ae50-3f8de70ed4fd",
   "metadata": {},
   "source": [
    "We're going to build an RNN that does predicts the next token, based on context tokens; which can be used to generate text from input string. This model is outlined in Chapter 3 of **The Hundred Page Language Models Book**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ba21951-ad8e-407c-8beb-6befb9853e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import math \n",
    "import os\n",
    "import random \n",
    "import re \n",
    "import tarfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer # pretrained tokenizer from Hugging Face Hub\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb99f3-7a89-4635-a866-20eaf52c71d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utilities\n",
    "\n",
    "Helper methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4e7fc5e-2659-4d9c-9619-21050dc69140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def check_file_exists(filename):\n",
    "    \"\"\"\n",
    "    Checks if a file exists in the current directory.\n",
    "    Args:\n",
    "        filename (str): Name of the file to check\n",
    "    Returns:\n",
    "        bool: True if file exists, False otherwise\n",
    "    \"\"\"\n",
    "    return os.path.exists(filename)\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def create_collate_fn(tokenizer):\n",
    "    \"\"\"\n",
    "    Creates a collate function for batching sequences of different lengths.\n",
    "    This function pads shorter sequences to match the longest sequence in the batch.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer object containing padding token information\n",
    "\n",
    "    Returns:\n",
    "        function: Collate function that handles padding in batches\n",
    "    \"\"\"\n",
    "    def collate_fn(batch):\n",
    "        # Separate inputs and targets from batch\n",
    "        input_seqs, target_seqs = zip(*batch)\n",
    "        # Get padding token ID from tokenizer\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        # Pad input sequences to same length\n",
    "        input_padded = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=pad_index)\n",
    "        # Pad target sequences to same length\n",
    "        target_padded = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=pad_index)\n",
    "        return input_padded, target_padded\n",
    "    return collate_fn\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def download_and_prepare_data(url, batch_size, tokenizer, max_length=30):\n",
    "    \"\"\"\n",
    "    Main function to handle the complete data preparation pipeline.\n",
    "    Downloads data, extracts it, and creates necessary dataset objects.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL where the dataset archive can be downloaded\n",
    "        batch_size (int): Batch size for data loading\n",
    "        tokenizer: Tokenizer object for text processing\n",
    "        max_length (int): Maximum sequence length for tokenization (default: 30)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataloader, test_dataloader) - Ready-to-use data loaders\n",
    "    \"\"\"\n",
    "    # Step 1: Download dataset archive from URL\n",
    "    filename = download_file(url)\n",
    "\n",
    "    # Step 2: Extract training and test files from archive\n",
    "    train_file, test_file = extract_dataset(filename)\n",
    "\n",
    "    # Step 3: Create dataset objects for streaming data\n",
    "    train_dataset, test_dataset = create_datasets(train_file, test_file, tokenizer, max_length)\n",
    "\n",
    "    # Step 4: Create function to handle batch creation\n",
    "    collate_fn = create_collate_fn(tokenizer)\n",
    "\n",
    "    # Step 5: Create and return data loaders\n",
    "    return create_dataloaders(train_dataset, test_dataset, batch_size, collate_fn)\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def download_file(url):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL if it doesn't exist locally.\n",
    "    Uses a custom User-Agent to help prevent download blocks.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the file to download\n",
    "    Returns:\n",
    "        str: Name of the downloaded file (\"news.tar.gz\")\n",
    "    \"\"\"\n",
    "    # Always use news.tar.gz as the filename, regardless of URL\n",
    "    filename = \"news.tar.gz\"\n",
    "\n",
    "    if not check_file_exists(filename):\n",
    "        print(f\"Downloading dataset from {url}...\")\n",
    "        req = urllib.request.Request(\n",
    "            url,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        )\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            with open(filename, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        print(\"Download completed.\")\n",
    "    else:\n",
    "        print(f\"{filename} already downloaded.\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def extract_dataset(filename):\n",
    "    \"\"\"\n",
    "    Extracts train.txt and test.txt from the downloaded archive.\n",
    "    Includes debug information about archive contents.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the archive file\n",
    "    Returns:\n",
    "        tuple: Paths to extracted train and test files\n",
    "    \"\"\"\n",
    "    data_dir = os.path.join(os.path.dirname(filename), \"news\")\n",
    "    train_path = os.path.join(data_dir, \"train.txt\")\n",
    "    test_path = os.path.join(data_dir, \"test.txt\")\n",
    "\n",
    "    if check_file_exists(train_path) and check_file_exists(test_path):\n",
    "        print(\"Data files already extracted.\")\n",
    "        return train_path, test_path\n",
    "\n",
    "    print(\"\\nListing archive contents:\")\n",
    "    with tarfile.open(filename, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            print(f\"Archive member: {member.name}\")\n",
    "\n",
    "        print(\"\\nExtracting files...\")\n",
    "        # Extract to current directory first\n",
    "        tar.extractall('.')\n",
    "\n",
    "    if not (check_file_exists(train_path) and check_file_exists(test_path)):\n",
    "        raise FileNotFoundError(f\"Required files not found in the archive. Please check the paths above.\")\n",
    "\n",
    "    print(\"Extraction completed.\")\n",
    "    return train_path, test_path\n",
    "\n",
    "    \n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def create_datasets(train_file, test_file, tokenizer, max_length=30):\n",
    "    \"\"\"\n",
    "    Creates IterableTextDataset objects for training and testing.\n",
    "    These datasets will stream data from disk instead of loading it all into memory.\n",
    "\n",
    "    Args:\n",
    "        train_file (str): Path to training data file\n",
    "        test_file (str): Path to test data file\n",
    "        tokenizer: Tokenizer object for text processing\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset) - Dataset objects for training and testing\n",
    "    \"\"\"\n",
    "    # Create training dataset\n",
    "    train_dataset = IterableTextDataset(train_file, tokenizer, max_length)\n",
    "    # Create test dataset\n",
    "    test_dataset = IterableTextDataset(test_file, tokenizer, max_length)\n",
    "\n",
    "    # Print dataset sizes\n",
    "    print(f\"Training sentences: {len(train_dataset)}\")\n",
    "    print(f\"Test sentences: {len(test_dataset)}\")\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "    \n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def create_dataloaders(train_dataset, test_dataset, batch_size, collate_fn):\n",
    "    \"\"\"\n",
    "    Creates DataLoader objects for efficient data iteration.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: Training dataset\n",
    "        test_dataset: Test dataset\n",
    "        batch_size (int): Number of sequences per batch\n",
    "        collate_fn: Function to handle padding and batch creation\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_dataloader, test_dataloader) - DataLoader objects for\n",
    "               iterating over batches of data with proper padding\n",
    "    \"\"\"\n",
    "    # Create training data loader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,    # Function to handle padding\n",
    "        num_workers=0             # Number of worker processes (0 = single process)\n",
    "    )\n",
    "    # Create test data loader\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "class IterableTextDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    An iterable dataset for processing text data in a memory-efficient way.\n",
    "    Instead of loading all data into memory, it streams data from disk.\n",
    "    Inherits from PyTorch's IterableDataset for streaming support.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file containing sentences\n",
    "        tokenizer: Tokenizer object for converting text to tokens\n",
    "        max_length (int): Maximum sequence length to process (default: 30)\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, tokenizer, max_length=30):\n",
    "        # Store file path for reading data\n",
    "        self.file_path = file_path\n",
    "        # Store tokenizer for text processing\n",
    "        self.tokenizer = tokenizer\n",
    "        # Set maximum sequence length to truncate long sequences\n",
    "        self.max_length = max_length\n",
    "        self._count_sentences()\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Creates an iterator over the dataset.\n",
    "        This method is called when iterating over the dataset.\n",
    "\n",
    "        Yields:\n",
    "            tuple: (input_sequence, target_sequence) pairs for language modeling\n",
    "                  input_sequence is the sequence up to the last token\n",
    "                  target_sequence is the sequence shifted one position right\n",
    "        \"\"\"\n",
    "        # Open file in read mode with UTF-8 encoding\n",
    "        with open(self.file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            # Process each line (sentence) in the file\n",
    "            for line in f:\n",
    "                # Remove leading/trailing whitespace\n",
    "                sentence = line.strip()\n",
    "                # Replace all numbers with ### placeholder\n",
    "                # This reduces vocabulary size and helps model generalize\n",
    "                sentence = re.sub(r\"\\d+\", \"###\", sentence)\n",
    "\n",
    "                # Convert sentence to token IDs\n",
    "                encoded_sentence = self.tokenizer.encode(\n",
    "                    sentence,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True\n",
    "                )\n",
    "\n",
    "                # Only use sequences with at least 2 tokens\n",
    "                # (need at least one input and one target token)\n",
    "                if len(encoded_sentence) >= 2:\n",
    "                    # Input is all tokens except last\n",
    "                    input_seq = encoded_sentence[:-1]\n",
    "                    # Target is all tokens except first\n",
    "                    target_seq = encoded_sentence[1:]\n",
    "                    # Convert to PyTorch tensors and yield\n",
    "                    yield torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self._num_sentences\n",
    "\n",
    "    def _count_sentences(self):\n",
    "        print(f\"Counting sentences in {self.file_path}...\")\n",
    "        with open(self.file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            self._num_sentences = sum(1 for _ in f)\n",
    "        print(f\"Found {self._num_sentences} sentences in {self.file_path}.\")\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Initializes model weights using Xavier uniform initialization for multi-dimensional\n",
    "    parameters and uniform initialization for biases and other 1D parameters.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): PyTorch model whose weights need to be initialized\n",
    "    \"\"\"\n",
    "    # Loop through all named parameters in the model\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check if parameter has more than 1 dimension (e.g., weight matrices)\n",
    "        if param.dim() > 1:\n",
    "            # Use Xavier uniform initialization for weight matrices\n",
    "            # This helps prevent vanishing/exploding gradients by keeping the variance constant\n",
    "            nn.init.xavier_uniform_(param)\n",
    "        else:\n",
    "            # For 1D parameters (like biases), use simple uniform initialization\n",
    "            nn.init.uniform_(param)\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def compute_loss_and_perplexity(model, dataloader, tokenizer, criterion, device, max_sentences=1000):\n",
    "    \"\"\"\n",
    "    Evaluates model performance by computing loss and perplexity on data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The language model to evaluate\n",
    "        dataloader (DataLoader): Data loader containing batched sequences\n",
    "        tokenizer: Tokenizer for handling special tokens like padding\n",
    "        criterion: Loss function (usually CrossEntropyLoss)\n",
    "        device: Device to run computation on (cuda/cpu)\n",
    "        max_sentences (int): Maximum number of sentences to evaluate (default: 1000)\n",
    "                           Limits evaluation to a subset for faster validation\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, perplexity)\n",
    "               - average_loss: Mean loss per token (excluding padding)\n",
    "               - perplexity: exp(average_loss), lower is better\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode (disables dropout, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for loss calculation\n",
    "    total_loss = 0.0          # Accumulator for total loss across all batches\n",
    "    total_tokens = 0          # Counter for total number of tokens (excluding padding)\n",
    "    sentences_processed = 0    # Counter for number of sentences processed\n",
    "\n",
    "    # Disable gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        # Iterate through data with progress bar\n",
    "        for input_seq, target_seq in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            # Move input and target sequences to specified device\n",
    "            input_seq = input_seq.to(device)      # Shape: (batch_size, seq_len)\n",
    "            target_seq = target_seq.to(device)    # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Get current batch size (might be smaller for last batch)\n",
    "            batch_size_current = input_seq.size(0)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            logits = model(input_seq)             # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # Reshape logits and target for loss calculation\n",
    "            logits = logits.reshape(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n",
    "            target = target_seq.reshape(-1)              # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Create mask to exclude padding tokens\n",
    "            mask = target != tokenizer.pad_token_id\n",
    "\n",
    "            # Compute loss only on non-padded tokens\n",
    "            loss = criterion(logits[mask], target[mask])\n",
    "\n",
    "            # Update counters\n",
    "            loss_value = loss.item() * mask.sum().item()  # Total loss for this batch\n",
    "            total_loss += loss_value                      # Accumulate batch loss\n",
    "            total_tokens += mask.sum().item()             # Count non-padding tokens\n",
    "\n",
    "            # Update sentence counter and check if we've reached maximum\n",
    "            sentences_processed += batch_size_current\n",
    "            if sentences_processed >= max_sentences:\n",
    "                break\n",
    "\n",
    "    # Calculate final metrics\n",
    "    average_loss = total_loss / total_tokens           # Normalize loss by number of tokens\n",
    "    perplexity = math.exp(average_loss)               # Convert loss to perplexity\n",
    "\n",
    "    return average_loss, perplexity\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def perform_model_evaluation(model, test_dataloader, criterion, tokenizer, device, contexts):\n",
    "    \"\"\"\n",
    "    Perform evaluation of the model including loss calculation, perplexity, and text generation.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        test_dataloader: DataLoader containing test/validation data\n",
    "        criterion: Loss function\n",
    "        tokenizer: Tokenizer for text generation\n",
    "        device: Device to run computations on (cuda/cpu)\n",
    "        contexts: List of context strings for text generation\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, perplexity)\n",
    "    \"\"\"\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Compute metrics\n",
    "    average_loss, perplexity = compute_loss_and_perplexity(\n",
    "        model, test_dataloader, tokenizer, criterion, device, max_sentences=1000\n",
    "    )\n",
    "\n",
    "    print(f\"Validation Average Loss: {average_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Generate text using the contexts\n",
    "    print(\"Generating text based on contexts using generate_text:\\n\")\n",
    "    for context in contexts:\n",
    "        generated_text = generate_text(\n",
    "            model=model,          # The loaded language model\n",
    "            start_string=context, # Context to continue\n",
    "            tokenizer=tokenizer,  # Tokenizer for text conversion\n",
    "            device=device,        # CPU or GPU device\n",
    "            max_length=50         # Maximum length of generated sequence\n",
    "        )\n",
    "        print(f\"\\nContext: {context}\")\n",
    "        print(f\"\\nGenerated text: {generated_text}\\n\")\n",
    "\n",
    "    return average_loss, perplexity\n",
    "\n",
    "\n",
    "#\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/news_RNN_language_model.ipynb\n",
    "#\n",
    "def generate_text(model, start_string, tokenizer, device, max_length=50):\n",
    "    \"\"\"\n",
    "    Generates text continuation from a given start string using greedy decoding.\n",
    "    This method always chooses the most likely next token.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained language model\n",
    "        start_string (str): Initial text to continue from\n",
    "        tokenizer: Tokenizer for text processing\n",
    "        device: Device to run generation on (cuda/cpu)\n",
    "        max_length (int): Maximum length of generated sequence\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text continuation\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start string to token ids and move to device\n",
    "    # return_tensors=\"pt\" returns PyTorch tensor instead of list\n",
    "    tokens = tokenizer.encode(start_string, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n",
    "\n",
    "    # Initialize generated sequence with input tokens\n",
    "    generated = tokens\n",
    "\n",
    "    # Generate new tokens one at a time\n",
    "    for _ in range(max_length):\n",
    "        # Get model's predictions\n",
    "        output = model(generated)                    # Shape: (1, seq_len, vocab_size)\n",
    "        # Get logits for the next token (last position)\n",
    "        next_token_logits = output[0, -1, :]        # Shape: (vocab_size)\n",
    "\n",
    "        # Choose token with highest probability (greedy decoding)\n",
    "        # unsqueeze twice to match expected shape (1, 1)\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Add new token to generated sequence\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "\n",
    "        # Stop if end of sequence token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Convert token ids back to text\n",
    "    generated_text = tokenizer.decode(generated.squeeze().tolist())\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56bf02-8e93-4522-8de3-46ec88bcf72b",
   "metadata": {},
   "source": [
    "## Encodings\n",
    "\n",
    "Before that, let's test out PyTorch embeddings module. This is basically a lookup table that generates random embeddings for words; i.e., it maps a vocabulary of size `num_embeddings` to a lower set of dimensions of size `embedding_dim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a2c64c6-eb92-44e4-9235-7a2fccd87585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 3, padding_idx=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=0)\n",
    "\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58454b13-9808-41ad-9754-eb5e867c8b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 2.4208,  3.1508, -0.6206],\n",
       "        [ 0.1543,  1.1907,  0.4148]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.tensor([0, 2, 4])\n",
    "emb(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ca984c-dab9-49ec-9b0a-94e4252c9e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [-0.3132,  0.2013, -0.4634],\n",
       "        [ 1.6002, -0.7654, -0.1727]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.tensor([0,1,3])\n",
    "emb(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "392cc418-d937-47b2-85e2-cbed4e08c041",
   "metadata": {},
   "source": [
    "## Define the RNN\n",
    "\n",
    "Here's the architecture:\n",
    "\n",
    "<img src=\"../images/rnn-architecture.png\" style=\"width:800px; border: 1px solid #000;\"/>\n",
    "\n",
    "Note that at training time, we define the number of vertical layers (which make up the Elman RNN) - but at inference time, its the **sequence length** that defines the \"width\" of this neural network above. I.e., the one Elman RNN of n layers is recurrent, and each token will result in another pass through the network and produce another value of `h`.\n",
    "\n",
    "Steps:\n",
    "1. First, we'll define an **Elman RNN Unit**, which represents a single node (single blue box above)\n",
    "2. Then, we'll define an **Elman RNN**, which is made up n layers of RNN units\n",
    "3. Then, we'll define the RNN language model, which include the Elman RNN as well as embedding and linear output layers\n",
    "\n",
    "After this section, we'll train and evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e068d8-5b47-42e9-9ead-ba9078ee4c6a",
   "metadata": {},
   "source": [
    "### Elman RNN unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b410c7-5af8-42b7-a075-21564bbfeb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNNUnit(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        # U learned parameters \n",
    "        self.Uh = nn.Parameter(torch.randn(emb_dim, emb_dim))\n",
    "        # W learned parameters\n",
    "        self.Wh = nn.Parameter(torch.randn(emb_dim, emb_dim))\n",
    "        self.b = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        #\n",
    "        # tanh activation fn, range [-1,1], often used for hidden layers in language models\n",
    "        # \n",
    "        #     Computes Wx + Yh + b\n",
    "        #\n",
    "        return torch.tanh(x @ self.Wh + h @ self.Uh + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e637c-32e9-4459-a32b-85d2c6463672",
   "metadata": {},
   "source": [
    "### Elman RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea711ce2-0a03-490b-9166-495a94fc9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_units = nn.ModuleList(\n",
    "            [ElmanRNNUnit(emb_dim) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        #\n",
    "        # initial h for first token is 0s, subsequent will be outputs from prev token.\n",
    "        #    (hence, recurrant)\n",
    "        # \n",
    "        h_prev = [ # forward any device (e.g., cpu, gpu, tpu) settings\n",
    "            torch.zeros(batch_size, emb_dim, device=x.device) for _ in range(self.num_layers)\n",
    "        ]\n",
    "        outputs = []\n",
    "        # for each token in sequence...\n",
    "        for t in range(seq_len):\n",
    "            input_t = x[:, t]\n",
    "            #print(f'DEBUG: input_t = {input_t}')\n",
    "            # pass the token through each layer (RNN unit)...\n",
    "            for l, rnn_unit in enumerate(self.rnn_units):\n",
    "                # TODO: update hidden state of h???\n",
    "                h_prev[l] = rnn_unit(input_t, h_prev[1])\n",
    "                # input for next layer\n",
    "                input_t = h_prev[l]\n",
    "            outputs.append(input_t)\n",
    "        return torch.stack(outputs, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb644afd-d535-4b63-bf1a-4e3aa490f36b",
   "metadata": {},
   "source": [
    "### RNN language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4163b49-eb72-4c17-94af-6b56302f0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_layers, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            emb_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        self.rnn = ElmanRNN(emb_dim, num_layers)\n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        embeddings = self.embedding(x)\n",
    "        rnn_output = self.rnn(embeddings)\n",
    "        logits = self.fc(rnn_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa17a8a6-1568-4975-9d6e-45b697f23818",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13195826-641a-43a5-b64d-18100ed9095f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32011"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll use the Phi 3.5 mini tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "vocab_size = len(tokenizer)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a8920fb-b99f-42b9-a752-f19645a7c043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens = [512, 6124, 304, 9348, 16226, 304, 278, 20042, 26504, 443, 8302, 4447, 368]\n",
      "token=512 value=In\n",
      "token=6124 value=addition\n",
      "token=304 value=to\n",
      "token=9348 value=sending\n",
      "token=16226 value=aid\n",
      "token=304 value=to\n",
      "token=278 value=the\n",
      "token=20042 value=struggling\n",
      "token=26504 value=economy\n",
      "token=443 value=un\n",
      "token=8302 value=famil\n",
      "token=4447 value=iar\n",
      "token=368 value=ly\n"
     ]
    }
   ],
   "source": [
    "# Test out the tokenizer\n",
    "tokens = tokenizer.encode(\n",
    "    \"In addition to sending aid to the struggling economy unfamiliarly\",\n",
    "    max_length=50,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(f'tokens = {tokens}')\n",
    "\n",
    "for token in tokens:\n",
    "    print(f'token={token} value={tokenizer.decode([token])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5130898f-b999-44d5-b9fd-8c4349b3971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "emb_dim = 128\n",
    "num_layers = 2\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1 \n",
    "context_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "869040d0-7d11-4e0e-b57a-d0562ea02c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news.tar.gz already downloaded.\n",
      "Data files already extracted.\n",
      "Counting sentences in news/train.txt...\n",
      "Found 22034911 sentences in news/train.txt.\n",
      "Counting sentences in news/test.txt...\n",
      "Found 449693 sentences in news/test.txt.\n",
      "Training sentences: 22034911\n",
      "Test sentences: 449693\n",
      "Total of 172148 batches in training dataset.\n"
     ]
    }
   ],
   "source": [
    "# fetch and prepare training and validation data\n",
    "data_url = \"https://www.thelmbook.com/data/news\"\n",
    "train_loader, test_loader = download_and_prepare_data(\n",
    "    data_url, batch_size, tokenizer\n",
    ")\n",
    "\n",
    "print(f'Total of {len(train_loader)} batches in training dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c04836-41c5-417f-977d-527234d3667e",
   "metadata": {},
   "source": [
    "Find a usable device, and default to CPU. On my MacBook Air M2:\n",
    "* If using CPU, training takes (10.68*100/.58)/60 = ~30.7hr\n",
    "* If using GPU (via MPS), training takes (10.68*100/.58)/60 = ~13.1hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c83a0f3-9bc6-49be-8d6d-d2209cdab532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def get_device_label():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "device = torch.device(get_device_label())\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# verify\n",
    "x = torch.ones(1, device=device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cc7413c-11ed-4759-951d-c7c96e69d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch=0] batch 0 of 172148 - 0.00% done - 0:00:00.013207 ellapsed\n",
      "[epoch=0] batch 1000 of 172148 - 0.58% done - 0:04:33.519919 ellapsed\n",
      "[epoch=0] batch 2000 of 172148 - 1.16% done - 0:09:08.341836 ellapsed\n",
      "[epoch=0] batch 3000 of 172148 - 1.74% done - 0:13:41.794978 ellapsed\n",
      "[epoch=0] batch 4000 of 172148 - 2.32% done - 0:18:16.350441 ellapsed\n",
      "[epoch=0] batch 5000 of 172148 - 2.90% done - 0:22:57.113256 ellapsed\n",
      "[epoch=0] batch 6000 of 172148 - 3.49% done - 0:27:35.927723 ellapsed\n",
      "[epoch=0] batch 7000 of 172148 - 4.07% done - 0:32:15.612264 ellapsed\n",
      "[epoch=0] batch 8000 of 172148 - 4.65% done - 0:36:50.383588 ellapsed\n",
      "[epoch=0] batch 9000 of 172148 - 5.23% done - 0:41:25.351493 ellapsed\n",
      "[epoch=0] batch 10000 of 172148 - 5.81% done - 1:01:50.740197 ellapsed\n",
      "[epoch=0] batch 11000 of 172148 - 6.39% done - 1:22:25.857147 ellapsed\n",
      "[epoch=0] batch 12000 of 172148 - 6.97% done - 1:52:30.279065 ellapsed\n",
      "[epoch=0] batch 13000 of 172148 - 7.55% done - 1:59:38.123273 ellapsed\n",
      "[epoch=0] batch 14000 of 172148 - 8.13% done - 2:04:16.144385 ellapsed\n",
      "[epoch=0] batch 15000 of 172148 - 8.71% done - 2:08:50.667436 ellapsed\n",
      "[epoch=0] batch 16000 of 172148 - 9.29% done - 2:13:24.117526 ellapsed\n",
      "[epoch=0] batch 17000 of 172148 - 9.88% done - 2:17:56.982645 ellapsed\n",
      "[epoch=0] batch 18000 of 172148 - 10.46% done - 2:22:31.456006 ellapsed\n",
      "[epoch=0] batch 19000 of 172148 - 11.04% done - 2:27:05.209262 ellapsed\n",
      "[epoch=0] batch 20000 of 172148 - 11.62% done - 2:31:37.981268 ellapsed\n",
      "[epoch=0] batch 21000 of 172148 - 12.20% done - 2:36:08.548121 ellapsed\n",
      "[epoch=0] batch 22000 of 172148 - 12.78% done - 2:40:41.185648 ellapsed\n",
      "[epoch=0] batch 23000 of 172148 - 13.36% done - 2:45:13.924212 ellapsed\n",
      "[epoch=0] batch 24000 of 172148 - 13.94% done - 2:49:47.656641 ellapsed\n",
      "[epoch=0] batch 25000 of 172148 - 14.52% done - 2:54:21.596894 ellapsed\n",
      "[epoch=0] batch 26000 of 172148 - 15.10% done - 2:58:54.051400 ellapsed\n",
      "[epoch=0] batch 27000 of 172148 - 15.68% done - 3:03:26.054775 ellapsed\n",
      "[epoch=0] batch 28000 of 172148 - 16.27% done - 3:07:58.251171 ellapsed\n",
      "[epoch=0] batch 29000 of 172148 - 16.85% done - 3:12:31.186110 ellapsed\n",
      "[epoch=0] batch 30000 of 172148 - 17.43% done - 3:17:01.395440 ellapsed\n",
      "[epoch=0] batch 31000 of 172148 - 18.01% done - 3:21:33.552725 ellapsed\n",
      "[epoch=0] batch 32000 of 172148 - 18.59% done - 3:26:05.267990 ellapsed\n",
      "[epoch=0] batch 33000 of 172148 - 19.17% done - 3:30:37.717216 ellapsed\n",
      "[epoch=0] batch 34000 of 172148 - 19.75% done - 3:35:09.309897 ellapsed\n",
      "[epoch=0] batch 35000 of 172148 - 20.33% done - 3:39:42.405207 ellapsed\n",
      "[epoch=0] batch 36000 of 172148 - 20.91% done - 3:44:13.843224 ellapsed\n",
      "[epoch=0] batch 37000 of 172148 - 21.49% done - 3:48:44.960312 ellapsed\n",
      "[epoch=0] batch 38000 of 172148 - 22.07% done - 3:53:14.894674 ellapsed\n",
      "[epoch=0] batch 39000 of 172148 - 22.65% done - 3:57:46.248906 ellapsed\n",
      "[epoch=0] batch 40000 of 172148 - 23.24% done - 4:02:20.290835 ellapsed\n",
      "[epoch=0] batch 41000 of 172148 - 23.82% done - 4:06:51.258239 ellapsed\n",
      "[epoch=0] batch 42000 of 172148 - 24.40% done - 4:11:25.352211 ellapsed\n",
      "[epoch=0] batch 43000 of 172148 - 24.98% done - 4:16:02.340785 ellapsed\n",
      "[epoch=0] batch 44000 of 172148 - 25.56% done - 4:20:35.812585 ellapsed\n",
      "[epoch=0] batch 45000 of 172148 - 26.14% done - 4:25:07.900736 ellapsed\n",
      "[epoch=0] batch 46000 of 172148 - 26.72% done - 4:29:41.983609 ellapsed\n",
      "[epoch=0] batch 47000 of 172148 - 27.30% done - 4:34:13.186738 ellapsed\n",
      "[epoch=0] batch 48000 of 172148 - 27.88% done - 4:38:45.323029 ellapsed\n",
      "[epoch=0] batch 49000 of 172148 - 28.46% done - 4:43:16.658845 ellapsed\n",
      "[epoch=0] batch 50000 of 172148 - 29.04% done - 4:47:46.839838 ellapsed\n",
      "[epoch=0] batch 51000 of 172148 - 29.63% done - 4:52:18.073006 ellapsed\n",
      "[epoch=0] batch 52000 of 172148 - 30.21% done - 4:56:47.897200 ellapsed\n",
      "[epoch=0] batch 53000 of 172148 - 30.79% done - 5:01:20.119829 ellapsed\n",
      "[epoch=0] batch 54000 of 172148 - 31.37% done - 5:05:51.429304 ellapsed\n",
      "[epoch=0] batch 55000 of 172148 - 31.95% done - 5:10:21.978239 ellapsed\n",
      "[epoch=0] batch 56000 of 172148 - 32.53% done - 5:14:52.835419 ellapsed\n",
      "[epoch=0] batch 57000 of 172148 - 33.11% done - 5:19:26.022131 ellapsed\n",
      "[epoch=0] batch 58000 of 172148 - 33.69% done - 5:23:59.606618 ellapsed\n",
      "[epoch=0] batch 59000 of 172148 - 34.27% done - 5:28:30.646165 ellapsed\n",
      "[epoch=0] batch 60000 of 172148 - 34.85% done - 5:33:01.385149 ellapsed\n",
      "[epoch=0] batch 61000 of 172148 - 35.43% done - 5:37:33.299840 ellapsed\n",
      "[epoch=0] batch 62000 of 172148 - 36.02% done - 5:42:04.408682 ellapsed\n",
      "[epoch=0] batch 63000 of 172148 - 36.60% done - 5:46:35.736410 ellapsed\n",
      "[epoch=0] batch 64000 of 172148 - 37.18% done - 5:51:08.844577 ellapsed\n",
      "[epoch=0] batch 65000 of 172148 - 37.76% done - 5:55:41.856844 ellapsed\n",
      "[epoch=0] batch 66000 of 172148 - 38.34% done - 6:00:15.815512 ellapsed\n",
      "[epoch=0] batch 67000 of 172148 - 38.92% done - 6:04:49.538367 ellapsed\n",
      "[epoch=0] batch 68000 of 172148 - 39.50% done - 6:09:21.014739 ellapsed\n",
      "[epoch=0] batch 69000 of 172148 - 40.08% done - 6:13:52.368420 ellapsed\n",
      "[epoch=0] batch 70000 of 172148 - 40.66% done - 6:18:24.313923 ellapsed\n",
      "[epoch=0] batch 71000 of 172148 - 41.24% done - 6:22:56.603868 ellapsed\n",
      "[epoch=0] batch 72000 of 172148 - 41.82% done - 6:27:28.594621 ellapsed\n",
      "[epoch=0] batch 73000 of 172148 - 42.41% done - 6:32:00.703719 ellapsed\n",
      "[epoch=0] batch 74000 of 172148 - 42.99% done - 6:36:30.457398 ellapsed\n",
      "[epoch=0] batch 75000 of 172148 - 43.57% done - 6:41:00.886237 ellapsed\n",
      "[epoch=0] batch 76000 of 172148 - 44.15% done - 6:45:30.535170 ellapsed\n",
      "[epoch=0] batch 77000 of 172148 - 44.73% done - 6:50:00.965461 ellapsed\n",
      "[epoch=0] batch 78000 of 172148 - 45.31% done - 6:54:30.417197 ellapsed\n",
      "[epoch=0] batch 79000 of 172148 - 45.89% done - 6:59:01.032209 ellapsed\n",
      "[epoch=0] batch 80000 of 172148 - 46.47% done - 7:03:32.487760 ellapsed\n",
      "[epoch=0] batch 81000 of 172148 - 47.05% done - 7:08:02.460119 ellapsed\n",
      "[epoch=0] batch 82000 of 172148 - 47.63% done - 7:12:33.937825 ellapsed\n",
      "[epoch=0] batch 83000 of 172148 - 48.21% done - 7:17:03.484931 ellapsed\n",
      "[epoch=0] batch 84000 of 172148 - 48.80% done - 7:21:34.616321 ellapsed\n",
      "[epoch=0] batch 85000 of 172148 - 49.38% done - 7:26:05.998575 ellapsed\n",
      "[epoch=0] batch 86000 of 172148 - 49.96% done - 7:30:37.755626 ellapsed\n",
      "[epoch=0] batch 87000 of 172148 - 50.54% done - 7:35:09.028785 ellapsed\n",
      "[epoch=0] batch 88000 of 172148 - 51.12% done - 7:39:40.022634 ellapsed\n",
      "[epoch=0] batch 89000 of 172148 - 51.70% done - 7:44:10.609687 ellapsed\n",
      "[epoch=0] batch 90000 of 172148 - 52.28% done - 7:48:41.271421 ellapsed\n",
      "[epoch=0] batch 91000 of 172148 - 52.86% done - 7:53:11.781387 ellapsed\n",
      "[epoch=0] batch 92000 of 172148 - 53.44% done - 7:57:42.770224 ellapsed\n",
      "[epoch=0] batch 93000 of 172148 - 54.02% done - 8:02:13.952607 ellapsed\n",
      "[epoch=0] batch 94000 of 172148 - 54.60% done - 8:06:45.164479 ellapsed\n",
      "[epoch=0] batch 95000 of 172148 - 55.19% done - 8:11:16.425389 ellapsed\n",
      "[epoch=0] batch 96000 of 172148 - 55.77% done - 8:15:47.651673 ellapsed\n",
      "[epoch=0] batch 97000 of 172148 - 56.35% done - 8:20:18.290674 ellapsed\n",
      "[epoch=0] batch 98000 of 172148 - 56.93% done - 8:24:48.954449 ellapsed\n",
      "[epoch=0] batch 99000 of 172148 - 57.51% done - 8:29:19.918843 ellapsed\n",
      "[epoch=0] batch 100000 of 172148 - 58.09% done - 8:33:52.659508 ellapsed\n",
      "[epoch=0] batch 101000 of 172148 - 58.67% done - 8:38:22.932166 ellapsed\n",
      "[epoch=0] batch 102000 of 172148 - 59.25% done - 8:42:54.075217 ellapsed\n",
      "[epoch=0] batch 103000 of 172148 - 59.83% done - 8:47:25.817587 ellapsed\n",
      "[epoch=0] batch 104000 of 172148 - 60.41% done - 8:51:56.404938 ellapsed\n",
      "[epoch=0] batch 105000 of 172148 - 60.99% done - 8:56:26.165841 ellapsed\n",
      "[epoch=0] batch 106000 of 172148 - 61.57% done - 9:00:59.073997 ellapsed\n",
      "[epoch=0] batch 107000 of 172148 - 62.16% done - 9:05:27.113786 ellapsed\n",
      "[epoch=0] batch 108000 of 172148 - 62.74% done - 9:09:59.072047 ellapsed\n",
      "[epoch=0] batch 109000 of 172148 - 63.32% done - 9:14:30.831591 ellapsed\n",
      "[epoch=0] batch 110000 of 172148 - 63.90% done - 9:19:01.721460 ellapsed\n",
      "[epoch=0] batch 111000 of 172148 - 64.48% done - 9:23:33.128992 ellapsed\n",
      "[epoch=0] batch 112000 of 172148 - 65.06% done - 9:28:07.499010 ellapsed\n",
      "[epoch=0] batch 113000 of 172148 - 65.64% done - 9:32:42.329986 ellapsed\n",
      "[epoch=0] batch 114000 of 172148 - 66.22% done - 9:37:14.736032 ellapsed\n",
      "[epoch=0] batch 115000 of 172148 - 66.80% done - 9:41:46.004012 ellapsed\n",
      "[epoch=0] batch 116000 of 172148 - 67.38% done - 9:46:16.647997 ellapsed\n",
      "[epoch=0] batch 117000 of 172148 - 67.96% done - 9:50:47.117182 ellapsed\n",
      "[epoch=0] batch 118000 of 172148 - 68.55% done - 9:55:18.158173 ellapsed\n",
      "[epoch=0] batch 119000 of 172148 - 69.13% done - 9:59:49.524008 ellapsed\n",
      "[epoch=0] batch 120000 of 172148 - 69.71% done - 10:04:21.156602 ellapsed\n",
      "[epoch=0] batch 121000 of 172148 - 70.29% done - 10:08:54.219615 ellapsed\n",
      "[epoch=0] batch 122000 of 172148 - 70.87% done - 10:13:25.041222 ellapsed\n",
      "[epoch=0] batch 123000 of 172148 - 71.45% done - 10:17:57.747226 ellapsed\n",
      "[epoch=0] batch 124000 of 172148 - 72.03% done - 10:22:29.623673 ellapsed\n",
      "[epoch=0] batch 125000 of 172148 - 72.61% done - 10:26:59.583904 ellapsed\n",
      "[epoch=0] batch 126000 of 172148 - 73.19% done - 10:31:32.128243 ellapsed\n",
      "[epoch=0] batch 127000 of 172148 - 73.77% done - 10:36:03.884497 ellapsed\n",
      "[epoch=0] batch 128000 of 172148 - 74.35% done - 10:40:34.915222 ellapsed\n",
      "[epoch=0] batch 129000 of 172148 - 74.94% done - 10:45:04.867587 ellapsed\n",
      "[epoch=0] batch 130000 of 172148 - 75.52% done - 10:49:36.220465 ellapsed\n",
      "[epoch=0] batch 131000 of 172148 - 76.10% done - 10:54:05.738272 ellapsed\n",
      "[epoch=0] batch 132000 of 172148 - 76.68% done - 10:58:36.154242 ellapsed\n",
      "[epoch=0] batch 133000 of 172148 - 77.26% done - 11:03:08.376800 ellapsed\n",
      "[epoch=0] batch 134000 of 172148 - 77.84% done - 11:07:40.430823 ellapsed\n",
      "[epoch=0] batch 135000 of 172148 - 78.42% done - 11:12:12.514140 ellapsed\n",
      "[epoch=0] batch 136000 of 172148 - 79.00% done - 11:16:44.938985 ellapsed\n",
      "[epoch=0] batch 137000 of 172148 - 79.58% done - 11:21:18.052191 ellapsed\n",
      "[epoch=0] batch 138000 of 172148 - 80.16% done - 11:25:50.081178 ellapsed\n",
      "[epoch=0] batch 139000 of 172148 - 80.74% done - 11:30:20.568159 ellapsed\n",
      "[epoch=0] batch 140000 of 172148 - 81.33% done - 11:34:54.167043 ellapsed\n",
      "[epoch=0] batch 141000 of 172148 - 81.91% done - 11:39:24.589920 ellapsed\n",
      "[epoch=0] batch 142000 of 172148 - 82.49% done - 11:43:55.753183 ellapsed\n",
      "[epoch=0] batch 143000 of 172148 - 83.07% done - 11:48:25.779607 ellapsed\n",
      "[epoch=0] batch 144000 of 172148 - 83.65% done - 11:52:57.010218 ellapsed\n",
      "[epoch=0] batch 145000 of 172148 - 84.23% done - 11:57:27.405455 ellapsed\n",
      "[epoch=0] batch 146000 of 172148 - 84.81% done - 12:01:59.279699 ellapsed\n",
      "[epoch=0] batch 147000 of 172148 - 85.39% done - 12:06:31.409270 ellapsed\n",
      "[epoch=0] batch 148000 of 172148 - 85.97% done - 12:11:02.039445 ellapsed\n",
      "[epoch=0] batch 149000 of 172148 - 86.55% done - 12:15:34.466227 ellapsed\n",
      "[epoch=0] batch 150000 of 172148 - 87.13% done - 12:20:04.730052 ellapsed\n",
      "[epoch=0] batch 151000 of 172148 - 87.72% done - 12:24:37.577685 ellapsed\n",
      "[epoch=0] batch 152000 of 172148 - 88.30% done - 12:29:10.206797 ellapsed\n",
      "[epoch=0] batch 153000 of 172148 - 88.88% done - 12:33:42.852521 ellapsed\n",
      "[epoch=0] batch 154000 of 172148 - 89.46% done - 12:38:14.292464 ellapsed\n",
      "[epoch=0] batch 155000 of 172148 - 90.04% done - 12:42:46.128721 ellapsed\n",
      "[epoch=0] batch 156000 of 172148 - 90.62% done - 12:47:17.849351 ellapsed\n",
      "[epoch=0] batch 157000 of 172148 - 91.20% done - 12:51:48.583954 ellapsed\n",
      "[epoch=0] batch 158000 of 172148 - 91.78% done - 12:56:18.850596 ellapsed\n",
      "[epoch=0] batch 159000 of 172148 - 92.36% done - 13:00:50.391682 ellapsed\n",
      "[epoch=0] batch 160000 of 172148 - 92.94% done - 13:05:22.108440 ellapsed\n",
      "[epoch=0] batch 161000 of 172148 - 93.52% done - 13:09:53.317005 ellapsed\n",
      "[epoch=0] batch 162000 of 172148 - 94.11% done - 13:14:24.677339 ellapsed\n",
      "[epoch=0] batch 163000 of 172148 - 94.69% done - 13:18:57.215203 ellapsed\n",
      "[epoch=0] batch 164000 of 172148 - 95.27% done - 13:23:28.875005 ellapsed\n",
      "[epoch=0] batch 165000 of 172148 - 95.85% done - 13:28:01.067940 ellapsed\n",
      "[epoch=0] batch 166000 of 172148 - 96.43% done - 13:32:33.878302 ellapsed\n",
      "[epoch=0] batch 167000 of 172148 - 97.01% done - 13:37:05.716378 ellapsed\n",
      "[epoch=0] batch 168000 of 172148 - 97.59% done - 13:41:37.751132 ellapsed\n",
      "[epoch=0] batch 169000 of 172148 - 98.17% done - 13:46:11.746907 ellapsed\n",
      "[epoch=0] batch 170000 of 172148 - 98.75% done - 13:50:44.016834 ellapsed\n",
      "[epoch=0] batch 171000 of 172148 - 99.33% done - 13:55:15.611270 ellapsed\n",
      "[epoch=0] batch 172000 of 172148 - 99.91% done - 13:59:47.334584 ellapsed\n",
      "CPU times: user 1h 38min 55s, sys: 23min 59s, total: 2h 2min 54s\n",
      "Wall time: 14h 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training starts here\n",
    "model = RecurrentLanguageModel(\n",
    "    vocab_size, emb_dim, num_layers, tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "initialize_weights(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        if idx % 1000 == 0:\n",
    "            ellapsed = datetime.now() - start\n",
    "            print(f'[epoch={epoch}] batch {idx} of {len(train_loader)} - {100*idx/len(train_loader):.2f}% done - {timedelta(seconds=ellapsed.total_seconds())} ellapsed')\n",
    "        \n",
    "        # prep X and y values \n",
    "        input_seq, target_seq = batch\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        batch_size_current, seq_len = input_seq.shape\n",
    "\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        #print(f'DEBUG: epoch={epoch} input_seq={input_seq} target_seq={target_seq} output={output}')\n",
    "        \n",
    "        # backpropogate error\n",
    "        output = output.reshape(batch_size_current * seq_len, vocab_size)b\n",
    "        target = target_seq.reshape(batch_size_current * seq_len)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "391bc9e8-eb16-4d75-a64f-920838a121a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_file = \"./rnn.pth\"\n",
    "torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40d046b6-d829-4aae-bb39-0c79f035d0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In addition to sending aid to the struggling country, the president offered to be a `` very strongest '' . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' ''\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate that we can load the model\n",
    "model2 = RecurrentLanguageModel( \n",
    "    vocab_size, emb_dim, num_layers, tokenizer.pad_token_id\n",
    ")\n",
    "model2.load_state_dict(torch.load(model_file, weights_only=True))\n",
    "model2.eval()\n",
    "model2.to(device)\n",
    "\n",
    "generate_text(\n",
    "    model=model2,\n",
    "    start_string=\"In addition to sending aid to the struggling country, the president offered to\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c178b-32c7-422d-aa48-c961c0eb1187",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daf88c94-a091-4fde-97a7-9e63d3627c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                | 0/3514 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average Loss: 4.3766, Perplexity: 79.56\n",
      "Generating text based on contexts using generate_text:\n",
      "\n",
      "\n",
      "Context: Moscow\n",
      "\n",
      "Generated text: Moscow , the former president , said he was not a member of the public . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said .\n",
      "\n",
      "\n",
      "Context: New York\n",
      "\n",
      "Generated text: New York City 's first-half strike was a ##-year-old boy , who was a member of the ##-year-old , who was a member of the ##-year-old , who was a member of the ##-year-\n",
      "\n",
      "\n",
      "Context: The hurricane\n",
      "\n",
      "Generated text: The hurricane , the ##-year-old was arrested on suspicion of murdering the police officer . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said . ' '' he said . '\n",
      "\n",
      "\n",
      "Context: The President\n",
      "\n",
      "Generated text: The President of the ####s of the ####s , which is also the same . ' '' the statement said . 's the . ' '' The FBI said the investigation was `` a matter of '' . '' 's . ' '' he said . '\n",
      "\n",
      "CPU times: user 2.18 s, sys: 4.42 s, total: 6.6 s\n",
      "Wall time: 38.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.376552756065364, 79.56328606131252)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "contexts = [\n",
    "    \"Moscow\",\n",
    "    \"New York\",\n",
    "    \"The hurricane\",\n",
    "    \"The President\"\n",
    "]\n",
    "perform_model_evaluation(model, test_loader, criterion, tokenizer, device, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d06a3a0-6a96-4806-a9e0-98da17f405dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

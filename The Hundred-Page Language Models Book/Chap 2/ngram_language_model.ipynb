{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21cbbe2b-2f12-4757-81f3-b74c1d4b1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gzip\n",
    "import io\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc048584-e9d1-49b5-b325-6d184e06d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count-based language model (n-gram model, e.g., n=3 for trigram)\n",
    "class CountLanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngram_counts = [{} for _ in range(n)]\n",
    "        self.total_unigrams = 0\n",
    "\n",
    "\n",
    "    # inference\n",
    "    def predict_next_token(self, context):\n",
    "        #\n",
    "        # uses backoff: \n",
    "        #   try largest trigram, and if found no matches, go to second largest trigram, etc...\n",
    "        #\n",
    "        for n in range(self.n, 1, -1): # n=3, n=2, then exit (we'll handle unigram specially)\n",
    "            if len(context) >= n - 1: # need to have enough tokens in context for this n-gram\n",
    "                context_n = tuple(context[-(n - 1):]) # grab the last n tokens from context (n-gram)\n",
    "                counts = self.ngram_counts[n - 1].get(context_n) # how many times did this n-gram appear?\n",
    "                if counts:\n",
    "                    return max(counts.items(), key = lambda x: x[1])[0] # if found, return the token that followed the n-gram the most\n",
    "\n",
    "        # special case: unigram\n",
    "        unigram_counts = self.ngram_counts[0].get(())\n",
    "        if unigram_counts:\n",
    "            return max(unigram_counts.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "        # no matches\n",
    "        return None\n",
    "\n",
    "\n",
    "    # training\n",
    "    def train(self, tokens):\n",
    "        self.total_unigrams = len(tokens)\n",
    "        for n in range(1, self.n + 1):\n",
    "            counts = self.ngram_counts[n - 1]\n",
    "            for i in range(len(tokens) - n + 1):\n",
    "                context = tuple(tokens[i:i + n - 1])\n",
    "                next_token = tokens[i + n - 1]\n",
    "                if context not in counts:\n",
    "                    counts[context] = defaultdict(int)\n",
    "                counts[context][next_token] += 1 \n",
    "\n",
    "\n",
    "    # performance metrics\n",
    "    def get_probability(self, token, context):\n",
    "        # n-gram n>1 (handle unigram separately)\n",
    "        for n in range(self.n, 1, -1):\n",
    "            if len(context) >= n - 1:\n",
    "                context_n = tuple(context[-(n - 1):])\n",
    "                counts = self.ngram_counts[n - 1].get(context_n)\n",
    "                if counts:\n",
    "                    total = sum(counts.values())\n",
    "                    count = counts.get(token, 0)\n",
    "                    if count > 0:\n",
    "                        return count / total\n",
    "        # unigram \n",
    "        unigram_counts = self.ngram_counts[0].get(())\n",
    "        count = unigram_counts.get(token, 0)\n",
    "        V = len(unigram_counts)\n",
    "        return (count + 1) / (self.total_unigrams + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94cacc4a-5089-40cb-885f-f05167bc92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/aburkov/theLMbook/blob/main/count_language_model.ipynb\n",
    "def download_corpus(url):\n",
    "    \"\"\"\n",
    "    Downloads and decompresses a gzipped corpus file from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the gzipped corpus file\n",
    "\n",
    "    Returns:\n",
    "        str: Decoded text content of the corpus\n",
    "\n",
    "    Raises:\n",
    "        HTTPError: If the download fails\n",
    "    \"\"\"\n",
    "    print(f\"Downloading corpus from {url}...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raises an exception for bad HTTP responses\n",
    "\n",
    "    print(\"Decompressing and reading the corpus...\")\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n",
    "        corpus = f.read().decode('utf-8')\n",
    "\n",
    "    print(f\"Corpus size: {len(corpus)} characters\")\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/count_language_model.ipynb\n",
    "def download_and_prepare_data(data_url):\n",
    "    \"\"\"\n",
    "    Downloads and prepares training and test data.\n",
    "\n",
    "    Args:\n",
    "        data_url (str): URL of the corpus to download\n",
    "\n",
    "    Returns:\n",
    "        tuple: (training_tokens, test_tokens) split 90/10\n",
    "    \"\"\"\n",
    "    # Download and extract the corpus\n",
    "    corpus = download_corpus(data_url)\n",
    "\n",
    "    # Convert text to tokens\n",
    "    tokens = tokenize(corpus)\n",
    "\n",
    "    # Split into training (90%) and test (10%) sets\n",
    "    split_index = int(len(tokens) * 0.9)\n",
    "    train_corpus = tokens[:split_index]\n",
    "    test_corpus = tokens[split_index:]\n",
    "\n",
    "    return train_corpus, test_corpus\n",
    "\n",
    "# source: https://github.com/aburkov/theLMbook/blob/main/count_language_model.ipynb\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text into words and periods.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "\n",
    "    Returns:\n",
    "        list: List of lowercase tokens matching words or periods\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\b[a-zA-Z0-9]+\\b|[.]\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b7676c3-accf-4e25-9b7a-7a4da3caef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading corpus from https://www.thelmbook.com/data/brown...\n",
      "Decompressing and reading the corpus...\n",
      "Corpus size: 6185606 characters\n"
     ]
    }
   ],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "# train the model\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "random.seed(42)\n",
    "n = 5 \n",
    "data_url = \"https://www.thelmbook.com/data/brown\"\n",
    "train_corpus, test_corpus = download_and_prepare_data(data_url) # TODO:\n",
    "\n",
    "model = CountLanguageModel(n)\n",
    "model.train(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6cff7727-3146-45bd-bd64-3b6389039814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: i will build a\n",
      "Next token: wall\n",
      "\n",
      "Context: the best place to\n",
      "Next token: live\n",
      "\n",
      "Context: she was riding a\n",
      "Next token: horse\n"
     ]
    }
   ],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "# test the model\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "contexts = [\n",
    "    \"i will build a\",\n",
    "    \"the best place to\",\n",
    "    \"she was riding a\"\n",
    "]\n",
    "\n",
    "for context in contexts:\n",
    "    words = tokenize(context)\n",
    "    next_word = model.predict_next_token(words)\n",
    "    print(f\"\\nContext: {context}\")\n",
    "    print(f\"Next token: {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb07bb0e-ee64-4dc7-a043-7e6358d701df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity on test corpus: 299.06\n"
     ]
    }
   ],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "# evaluate model performance\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "def compute_perplexity(model, tokens, context_size):\n",
    "    if not tokens:\n",
    "        return float('inf')\n",
    "    total_log_likelihood = 0\n",
    "    num_tokens = len(tokens)\n",
    "    for i in range(num_tokens):\n",
    "        context_start = max(0, i - context_size)\n",
    "        context = tuple(tokens[context_start:i])\n",
    "        word = tokens[i]\n",
    "        probability = model.get_probability(word, context)\n",
    "        total_log_likelihood += math.log(probability)\n",
    "    average_log_likelihood = total_log_likelihood / num_tokens\n",
    "    return math.exp(-average_log_likelihood)\n",
    "\n",
    "perplexity = compute_perplexity(model, test_corpus, n)\n",
    "print(f\"\\nPerplexity on test corpus: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "131a1b79-1dca-4471-bb9c-6b86a6a0c539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i will build a wall to keep the people in and added so long as people rebel we must not give up . martin called for patience on the part of the coaches and the players . we needed it and we got it . meek expressed particular gratification at the defensive performances of end happy nelson and halfback billy gannon . both turned in top jobs for the second straight game . nelson played magnificent football meek praised . he knocked down the interference and made key stops lots of times . and he caused the fumble that set up our touchdown .\n"
     ]
    }
   ],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "# generate conversation\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "tokens = tokenize(\"i will build a\")\n",
    "for i in range(100):\n",
    "    next_token = model.predict_next_token(tokens)\n",
    "    tokens.append(next_token)\n",
    "print(\" \".join(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

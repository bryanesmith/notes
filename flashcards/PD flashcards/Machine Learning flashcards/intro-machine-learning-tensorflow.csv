"gradient","the derivative of the error function"
"gradient descent","[1] prediction ŷ = w x + b, [2] coefficients w -> w + α (y - ŷ)x, [3] intercepts b -> b + α (y - ŷ), [4] repeat"
"mean absolute error","Error = (1/m) Σ(i=1..m) |y - ŷ|"
"mean squared error","Error = (1/2m) Σ(i=1..m) (y - ŷ)²"
"mini-batch gradient descent","Split data into many batches, and apply gradient descent."
"linear regression","Regression supporting linear space features. E.g., ŷ = w₁x + w₂"
"polynomial regression","Regression supporting polynomial space features. E.g., ŷ = w₁x³ + w₂x² + w₃x + w₄"
"regularization","factor in the complexity of the model into the error, in order to avoid overfitting the training data"
"L1 Regularization","Regularization where add the absolute value of the coefficients to the error"
"L2 Regularization","Regularization where add the squares of the coefficients to the error"
"feature scaling","converting data into common range of values"
"standardizing","Feature scaling where for each value, subtract mean and divide by standard deviation"
"normalizing","Feature scaling where for each value, data scaled between 0 and 1"
"step function","converts continuous number to 0 or 1"
"Formula for entropy","- Σ(i=1..m) pᵢ log₂(pᵢ)"
"Formula for information gain","Entropy(parent) - [p₁ Entropy(child₁) + p₂ Entropy(child₂)]"
"paired data (in pandas)","the features at `X.loc[i]` is associated with the label at `y.loc[i]"
"One-hot encoding","Convert categorical data to columns of numeric data made up of 0s and 1s"
"ham (messaging)","A message that's not spam."
"Bayes Theorem","P(A|R) = P(A) P(R|A) / P(R)"
"Naive Bayes algorithm","Find P(A|S₀,S₁,...) ∝ P(S₀|A) P(S₁|A) ... and P(B|S₀,S₁,...) ∝ P(S₀|B) P(S₁|B) ..., then make these add to 1"
"Why is Naive Bayes naive?","Because it assumes independence: P(A ∩ B) = P(A)P(B)"
"Bag of Words (BoW)","count the frequency of the words in given text"
"Multinomial Naive Bayes","Naive Bayes algorithm suitable for classification with discrete features"
"Gaussian Naive Bayes","Naive Bayes algorithm suitable for classification with continuous features"
"Support Vector Machines (SVM)","Classification algorithm that finds the boundary that maintains largest distance from points."
"SVM error function","C * Classification Error + Margin Error, where C is a hyperparameter"
"Classification Error (SVM)","Sum of distance for misclassified points to closest margin line; punishes misclassifications"
"Margin Error (SVM)","2/|W| for vector W; punishes small margins"
"norm of a vector","different ways to calculate the the length (or magnitudes) of vectors"
"L1 norm of a vector","Sum of the absolute values of the vector: l₁(v) = ||v||₁ = |a₁| + |a₂| + |a₃|"
"L2 norm of a vector","Square root of the squared vector values: l₂(v) = ||v||₂ = sqrt(a₁² + a₂² + a₃²)"
"tensor","In linear algebra, an array with more than two dimensions"
"kernel trick","lift data points to a higher dimension in order to make them linearly separable, then project that back to original dimensionality"
"linear kernel","Kernel that supports linear functions of the form `k(x,y) = xᵀy + c` (which includes {`x`, `y`})"
"polynomial kernel","Kernel that supports polynomial functions of the form `k(x,y) = (∝xᵀy + c)ᵈ` (a 2nd degree polynomial includes {`x`, `y`, `x²`, `y²`, `xy`})"
"radial basis function (rbf) kernel","Kernel that uses a density-based approach (closeness of points to each other) to classify otherwise difficult to classify data. Uses gamma hyperparameter to control fitting behavior."
"two categories of ensemble learning","bagging and boosting"
"bagging (Bootstrap Aggregating)","Build bunch of weak models using subset of data, then averaging or voting across models"
"boosting","when using a ensemble of models, set weight for each model based on their strengths"
"random forest algorithm","Bagging algorithm that builds many decision trees using subset of features, and run each observation across all trees and vote on prediction."
"weak learners","When using ensemble, these are the individual models (with high bias or variance) that only need to have slightly better than random performance."
"bias","Type of error that can lead to underfitting. (E.g., linear regression is high in this.)"
"variance","Type of error that can lead to overfitting. (E.g., decision tree is high in this.)"
"bootstrapping","Sampling data with replacement and fitting algorithm to sample data. (One way to introduce randomness into high variance algorithms prior to making ensemble.)"
"subsetting the features","Only use subset of features for each decision tree. (One way to introduce randomness into high variance algorithms prior to making ensemble.)"
"Adaboost","Boosting algorithm that iteratively creates weak models, putting increasing weight on misclassifications."
"fitting","Training the model using training data."
"scoring","Generating predictions by applying a fitted model to observation data."
"bias-variance tradeoff","Model should have enough degrees of freedom to resolve the underlying complexity of the data, but not too many degrees of freedom so it is robust and avoids high variance"
"No Free Lunch theorem","Controversial theorem that states the no algorithm is best suited for all possible scenarios, and the average performance of each will be equivalent. Each model's degree of bias will make it more suited to some data over others."
"confusion matrix","table used to store true and false positives/negatives, in order to help determine how well a model is doing"
"type 1 error","aka False Positive"
"type 2 error","aka False Negative"
"accuracy","(TP + TN) / (TP + FP + TN + FN). What percentage of data points did we classify correctly?"
"precision","TP / (TP + FP). Out of all data predicted to be data, how many are actually positive? Punishes false positives."
"recall","TP / (TP + FN). Out of all the positive data, how many were predicted to be positive? Punishes false negatives. (aka the 'true positive rate', or 'sensitivity'.)"
"harmonic mean","2xy / (x + y). Always less than arithmetic mean."
"F₁ Score","2 * Precision * Recall / (Precision + Recall). I.e., the harmonic mean of precision and recall."
"Fᵦ Score","(1 + β) * Precision * Recall / (β * Precision + Recall). Large β yields better recall, low β yields better precision."
"false positive rate","FP / (TN + FP)"
"Receiver Operator Characteristic (ROC) curve","Graph all points using the true positive rate and false negative rates as (x,y) coordinates, respectively. Closer the area under curve is to one, better the model is."
"R2 score","1 - mean_squared_error(regression_model) / mean_squared_error(simplest_regression_model). A good model should yield value closer to 1."

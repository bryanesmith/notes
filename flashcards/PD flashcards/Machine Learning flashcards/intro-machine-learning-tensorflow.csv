"gradient","the derivative of the error function"
"gradient descent","[1] prediction ŷ = w x + wb, [2] coefficients w -> w + α (y - ŷ)x, [3] intercepts b -> b + α (y - ŷ), [4] repeat"
"mean absolute error","Error = (1/m) Σ(i=1..m) |y - ŷ|"
"mean squared error","Error = (1/2m) Σ(i=1..m) (y - ŷ)²"
"mini-batch gradient descent","Split data into many batches, and apply gradient descent."
"linear regression","Regress supporting linear space features. E.g., ŷ = w₁x + w₂"
"polynomial regression","Regression supporting polynomial space features. E.g., ŷ = w₁x³ + w₂x² + w₃x + w₄"
"regularization","factor in the complexity of the model into the error, in order to avoid overfitting the training data"
"L1 Regularization","Regularization where add the absolute value of the coefficients to the error"
"L2 Regularization","Regularization where add the squares of the coefficients to the error"
"feature scaling","converting data into common range of values"
"standardizing","Feature scaling where for each value, subtract mean and divide by standard deviation"
"normalizing","Feature scaling where for each value, data scaled between 0 and 1"

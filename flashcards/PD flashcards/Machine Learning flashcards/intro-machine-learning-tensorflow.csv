"gradient","the derivative of the error function"
"gradient descent","[1] prediction ŷ = w x + wb, [2] coefficients w -> w + α (y - ŷ)x, [3] intercepts b -> b + α (y - ŷ), [4] repeat"
"mean absolute error","Error = (1/m) Σ(i=1..m) |y - ŷ|"
"mean squared error","Error = (1/2m) Σ(i=1..m) (y - ŷ)²"
"mini-batch gradient descent","Split data into many batches, and apply gradient descent."
"linear regression","Regress supporting linear space features. E.g., ŷ = w₁x + w₂"
"polynomial regression","Regression supporting polynomial space features. E.g., ŷ = w₁x³ + w₂x² + w₃x + w₄"
"regularization","factor in the complexity of the model into the error, in order to avoid overfitting the training data"
"L1 Regularization","Regularization where add the absolute value of the coefficients to the error"
"L2 Regularization","Regularization where add the squares of the coefficients to the error"
"feature scaling","converting data into common range of values"
"standardizing","Feature scaling where for each value, subtract mean and divide by standard deviation"
"normalizing","Feature scaling where for each value, data scaled between 0 and 1"
"step function","converts continuous number to 0 or 1"
"Formula for entropy","- Σ(i=1..m) pᵢ log₂(pᵢ)"
"Formula for information gain","Entropy(parent) - [p₁ Entropy(child₁) + p₂ Entropy(child₂)]"
"paired data (in pandas)","the features at `X.loc[i]` is associated with the label at `y.loc[i]"
"One-hot encoding","Convert categorical data to columns of numeric data made up of 0s and 1s"
"ham (messaging)","A message that's not spam."
"Bayes Theorem","P(A|R) = P(A) P(R|A) / P(R)"
"Naive Bayes algorithm","Find P(A|S₀,S₁,...) ∝ P(S₀|A) P(S₁|A) ... and P(B|S₀,S₁,...) ∝ P(S₀|B) P(S₁|B) ..., then make these add to 1"
"Why is Naive Bayes naive?","Because it assumes independence: P(A ∩ B) = P(A)P(B)"
"Bag of Words (BoW)","count the frequency of the words in given text"
"Multinomial Naive Bayes","Naive Bayes algorithm suitable for classification with discrete features"
"Gaussian Naive Bayes","Naive Bayes algorithm suitable for classification with continuous features"
"Accuracy","ratio of the number of correct predictions to the total number of predictions"
"Precision","ratio of true positives to all positives"
"Recall (sensitivity)","ratio of true positives to all the words that were positives (true positives + false negatives)"
"F1 score","weighted average of the precision and recall"
"Support Vector Machines (SVM)","Classification algorithm that finds the boundary that maintains largest distance from points."
"SVM error function","C * Classification Error + Margin Error, where C is a hyperparameter"
"Classification Error (SVM)","Sum of distance for misclassified points to closest margin line; punishes misclassifications"
"Margin Error (SVM)","2/|W| for vector W; punishes small margins"
"norm of a vector","different ways to calculate the the length (or magnitudes) of vectors"
"L1 norm of a vector","Sum of the absolute values of the vector: l₁(v) = ||v||₁ = |a₁| + |a₂| + |a₃|"
"L2 norm of a vector","Square root of the squared vector values: l₂(v) = ||v||₂ = sqrt(a₁² + a₂² + a₃²)"
"tensor","In linear algebra, an array with more than two dimensions"
"kernel trick","lift data points to a higher dimension in order to make them linearly separable, then project that back to original dimensionality"
"linear kernel","Kernel that supports linear functions of the form `k(x,y) = xᵀy + c` (which includes {`x`, `y`})"
"polynomial kernel","Kernel that supports polynomial functions of the form `k(x,y) = (∝xᵀy + c)ᵈ` (a 2nd degree polynomial includes {`x`, `y`, `x²`, `y²`, `xy`})"
"radial basis function (rbf) kernel","Kernel that uses a density-based approach (closeness of points to each other) to classify otherwise difficult to classify data. Uses gamma hyperparameter to control fitting behavior."

"gradient","the derivative of the error function"
"gradient descent","[1] prediction ŷ = w x + b, [2] coefficients w -> w + α (y - ŷ)x, [3] intercepts b -> b + α (y - ŷ), [4] repeat"
"mean absolute error","Error = (1/m) Σ(i=1..m) |y - ŷ|"
"mean squared error","Error = (1/2m) Σ(i=1..m) (y - ŷ)²"
"mini-batch gradient descent","Split data into many batches, and apply gradient descent."
"linear regression","Regression supporting linear space features. E.g., ŷ = w₁x + w₂"
"polynomial regression","Regression supporting polynomial space features. E.g., ŷ = w₁x³ + w₂x² + w₃x + w₄"
"regularization","factor in the complexity of the model into the error, in order to avoid overfitting the training data"
"feature scaling","converting data into common range of values"
"standardizing","Feature scaling where for each value, subtract mean and divide by standard deviation"
"normalizing","Feature scaling where for each value, data scaled between 0 and 1"
"step function","converts continuous number to 0 or 1"
"Formula for entropy","- Σ(i=1..m) pᵢ log₂(pᵢ)"
"Formula for information gain","Entropy(parent) - [p₁ Entropy(child₁) + p₂ Entropy(child₂)]"
"paired data (in pandas)","the features at `X.loc[i]` is associated with the label at `y.loc[i]"
"One-hot encoding","Convert categorical data to columns of numeric data made up of 0s and 1s"
"ham (messaging)","A message that's not spam."
"Bayes Theorem","P(A|R) = P(A) P(R|A) / P(R)"
"Naive Bayes algorithm","Find P(A|S₀,S₁,...) ∝ P(S₀|A) P(S₁|A) ... and P(B|S₀,S₁,...) ∝ P(S₀|B) P(S₁|B) ..., then make these add to 1"
"Why is Naive Bayes naive?","Because it assumes independence: P(A ∩ B) = P(A)P(B)"
"Bag of Words (BoW)","count the frequency of the words in given text"
"Multinomial Naive Bayes","Naive Bayes algorithm suitable for classification with discrete features"
"Gaussian Naive Bayes","Naive Bayes algorithm suitable for classification with continuous features"
"Support Vector Machines (SVM)","Classification algorithm that finds the boundary that maintains largest distance from points."
"SVM error function","C * Classification Error + Margin Error, where C is a hyperparameter"
"Classification Error (SVM)","Sum of distance for misclassified points to closest margin line; punishes misclassifications"
"Margin Error (SVM)","2/|W| for vector W; punishes small margins"
"norm of a vector","different ways to calculate the the length (or magnitudes) of vectors"
"L1 norm of a vector","Sum of the absolute values of the vector: l₁(v) = ||v||₁ = |a₁| + |a₂| + |a₃|"
"L2 norm of a vector","Square root of the squared vector values: l₂(v) = ||v||₂ = sqrt(a₁² + a₂² + a₃²)"
"tensor","In linear algebra, an n-dimensional array. This is a generalization of vectors and matrices."
"kernel trick","lift data points to a higher dimension in order to make them linearly separable, then project that back to original dimensionality"
"linear kernel","Kernel that supports linear functions of the form `k(x,y) = xᵀy + c` (which includes {`x`, `y`})"
"polynomial kernel","Kernel that supports polynomial functions of the form `k(x,y) = (∝xᵀy + c)ᵈ` (a 2nd degree polynomial includes {`x`, `y`, `x²`, `y²`, `xy`})"
"radial basis function (rbf) kernel","Kernel that uses a density-based approach (closeness of points to each other) to classify otherwise difficult to classify data. Uses gamma hyperparameter to control fitting behavior."
"two categories of ensemble learning","bagging and boosting"
"bagging (Bootstrap Aggregating)","Build bunch of weak models using subset of data, then averaging or voting across models"
"boosting","when using a ensemble of models, set weight for each model based on their strengths"
"random forest algorithm","Bagging algorithm that builds many decision trees using subset of features, and run each observation across all trees and vote on prediction."
"weak learners","When using ensemble, these are the individual models (with high bias or variance) that only need to have slightly better than random performance."
"bias","Type of error that can lead to underfitting. (E.g., linear regression is high in this.)"
"variance","Type of error that can lead to overfitting. (E.g., decision tree is high in this.)"
"bootstrapping","Sampling data with replacement and fitting algorithm to sample data. (One way to introduce randomness into high variance algorithms prior to making ensemble.)"
"subsetting the features","Only use subset of features for each decision tree. (One way to introduce randomness into high variance algorithms prior to making ensemble.)"
"Adaboost","Boosting algorithm that iteratively creates weak models, putting increasing weight on misclassifications."
"fitting","Training the model using training data."
"scoring","Generating predictions by applying a fitted model to observation data."
"bias-variance tradeoff","Model should have enough degrees of freedom to resolve the underlying complexity of the data, but not too many degrees of freedom so it is robust and avoids high variance"
"No Free Lunch theorem","Controversial theorem that states the no algorithm is best suited for all possible scenarios, and the average performance of each will be equivalent. Each model's degree of bias will make it more suited to some data over others."
"confusion matrix","table used to store true and false positives/negatives, in order to help determine how well a model is doing"
"type 1 error","aka False Positive"
"type 2 error","aka False Negative"
"accuracy","(TP + TN) / (TP + FP + TN + FN). What percentage of data points did we classify correctly?"
"precision","TP / (TP + FP). Out of all data predicted to be positive, how many are actually positive? (Punishes false positives.)"
"recall","TP / (TP + FN). Out of all the positive data, how many were predicted to be positive? Punishes false negatives. (aka the 'true positive rate', or 'sensitivity'.)"
"harmonic mean","2xy / (x + y). Always less than or equal to arithmetic mean."
"F₁ Score","2 * Precision * Recall / (Precision + Recall). I.e., the harmonic mean of precision and recall."
"Fᵦ Score","(1 + β) * Precision * Recall / (β * Precision + Recall). Large β yields better recall, low β yields better precision."
"false positive rate","FP / (TN + FP)"
"Receiver Operator Characteristic (ROC) curve","Graph all points using the true positive rate and false negative rates as (x,y) coordinates, respectively. Closer the area under curve is to one, better the model is."
"R2 score","1 - MSE(regression_model) / MSE(simplest_regression_model). A good model should yield value closer to 1."
"logarithmic transformation","Significantly reduces range of data to prevent negative impact on performance of models."
"categorical variables","non-numeric features"
"naive predictor","Purpose is to establish baseline performance of a model without intelligence."
"two steps in perceptron","1) linear function (ΣWᵢXᵢ + b, where X are inputs and W are input weights), 2) step function (WX + b > 0?)"
"perceptron algorithm","1) assign random weights (W) and intercept (b) to line, and classify every point; 2) for every misclassified point, adjust weights and intercept, applying learning rate; 3) repeat step 2"
"sigmoid function","σ(x) = 1/(1+e⁻ˣ)"
"softmax function","P(classᵢ) = e^Zᵢ / ∑ e^Zⱼ for all scores in Z. Functions as a multi-class sigmoid function."
"maximum likelihood","select the model that gives existing labels the highest probability"
"cross-entropy","-∑∑ yᵢⱼ ln(pᵢⱼ). how likely are the events, based on specified probabilities? (low values better than high values.)"
"gradient of the error function","▽E = (∂E/∂w₁, ..., ∂E/∂wᵢ, ∂E/∂b)"
"gradient descent step (for logistic regression)"," wᵢ′ <- wᵢ + α(y-ŷ)xᵢ, b′ <- b + α(y-ŷ)"
"epoch","what an iteration of gradient descent is called"
"multi-layer perceptrons","another name for 'neural networks'"
"3 layers of a neural network","input layer, hidden layer, output layer"
"multi-class neural network","classifier neural network with multiple outputs"
"deep neural network","neural network with more than one hidden layer"
"feedforward","process neural networks use to turn the inputs into output(s)"
"feedforward formula when 1 hidden layer","ŷ = σ W⁽²⁾ ⚬ σ W⁽¹⁾x"
"chain rule","∂/∂x p ⚬ q(x) = ∂p/∂q * ∂q/∂x"
"Sum of Squared Errors","E = 1/2 ∑[yⱼ - ŷⱼ]²"
"row vectors","1 x n vectors"
"column vector","n x 1 vectors"
"vanishing gradient problem","when using backpropagation with deep neural networks, weights steps increasingly shrink closer to the inputs"
"cross-validation vs testing","procedure for selecting which model to use vs procedure for testing the chosen model's performance"
"k-fold cross validation","divide data into k buckets, and train model k times, using one bucket for testing and k-1 buckets for training; average the scores across all trained models"
"learning curves","graph errors (y-axis) for both training and cross-validation as increase number of data points (x-axis), in order to flag underfitting and overfitting"
"hyperparameter","characteristic of the model external to the model, and whose value cannot be determined by the data"
"parameter","characteristic of the model internal to the model, and whose value can be estimated by the data"
"grid search","hyperparameter turning by exhaustively training and testing a model using every hyperparameter combination"
"model complexity graph","graph errors (y-axis) for both training and testing against the model complexity (x-axis), in order to flag underfitting and overfitting"
"curse of dimensionality","as the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially"
"early stopping","training the neural network up until the testing error starts to increase"
"L1 regularization","adding λ(|w₁| + ... + |wᵢ|) term to error function"
"L2 regularization","adding λ(w₁² + ... + wᵢ²) term to error function"
"η","lowercase eta"
"δ","lowercase delta"
"λ","lowercase lambda"
"∇","symbol called 'Nabla', meaning 'harp'. Used for gradients."
"Δ","uppercase delta"
"transfer learning","storing knowledge gained while solving one problem and applying it to a different but related problem"
"tensors","an n-dimensional array; generalization of vectors and matrices"
"Keras","an interface for TensorFlow (and other libraries) for working with deep neural networks"
"tensorflow_datasets","repository of datasets for use with TensorFlow, including the MNIST dataset"
"MNIST dataset","dataset containing 28x28 pixel images of handwritten digits"
"dense network","a network that is fully connected"
"convolutional neural network (CNN)","combines convolutional, pooling, and fully-connected layers in a network architecture that is particularly effective for image classification"
"convolution layer","layers of CNN that apply filters across images"
"pooling layers","layers of CNN that apply thresholds to the output of a convolution layer"
"fully-connected layers","layers of CNN that take output from final pooling layers and perform classifications (by outputting multi-class probabilities, and caller picks the largest)"
"ImageNet","massive labeled image dataset that assigns hundreds of thousands of images to nouns in WordNet dataset"
"TensorFlow Hub","Tensorflow repository for sharing networks that can be reused for transfer learning"
"k-means clustering","clustering approach that groups data points into specified number different groups, utilizing randomly placed centroids"
"elbow method","plotting the number of clusters K (x-axis) against the avg distance to cluster center (y-axis), and looking for the point where the decrease in distance becomes small"
"min-max scalar","scaling data as percentage of maximum value"
"standard scalar","scaling data so it has mean 0 and variance 1"
"dendrograms","tree diagram, especially one showing taxonomic relationships"
"agglomerative clustering","assuming every data point is a cluster, building up clusters of clusters from ground up"
"single-link clustering","hierarchical (agglomerative) clustering technique using the distance of the closest points across clusters"
"complete link clustering","hierarchical (agglomerative) clustering technique using the distance of the furthest points across clusters"
"average link clustering","hierarchical (agglomerative) clustering technique using the average distance between points across clusters"
"Ward's method","hierarchical (agglomerative) clustering method that minimizes variance when clustering two clusters"
"Adjusted Rand Score","Similarity measurement between two clusters, ranging from -1 (low similarity) to 1 (identical)"
"DBSCAN","density-based clustering algorithm that can find clusters while labeling some points as noise and excluding from clusters, using maximum search distance and minimum cluster size"
"soft clustering","every data point belongs to every cluster, but with differing levels of membership"
"Gaussian Mixture Model","soft clustering technique using expectation-maximization algorithm that assumes clusters of data following statistical distributions"
"multivariate Gaussian distribution","Gaussian distribution with multiple variables (composed of multiple distributions, one per dimension)"
"expectation-maximization algorithm","1) initialize K Gaussian distributions; 2) soft-cluster data; 3) re-estimate the Gaussians; 4) test for convergence (or go back to step 2)"
"log-likelihood function","function that produces values that indicate how likely our distributions produced observed data, which we want to maximize."
"feature extraction","transforming data to create novel or useful features"
"external indices","indices used for validating clusters with labeled data"
"internal indices","measure fit between data and clustering structure based on data itself"
"relative indices","measures which of clusters is better (all internal indices can be used for this purpose)"
"2 characteristics for comparing clusters","compactness (points are close together) and separability (clusters are further apart)"
"silhouette coefficient","an internal validation technique yielding a value between -1 and 1, utilizing the average distance to samples in cluster as well as average distance to points in closest cluster"
"PairGrid","plotting every pair of features against each other as subplots"
"latent feature","features not directly observed, but underlies observed features (e.g. size of house, quality of neighborhood)"
"feature reduction","identifying subset of data that is most useful"
"feature extraction","constructing new latent features from original dataset"
"principal components","type of latent feature using linear combinations (projection onto lower dimensional space) of original features in order to retain most information in original data"
"Random Projection","given acceptable loss of variance, generates an unspecified number of random lines to reduce a high dimensional space to a lower one"
"blind source separation problem","when multiple independent sources of data are combined together (e.g., hear a piano, cello, TV), and we want to separate them out."
"Independent Component Analysis","finds independent components in a dataset, addressing blind source separation problems. Requires as many signals (e.g., audio waveform) as components."

"gradient","the derivative of the error function"
"gradient descent","[1] prediction ŷ = w x + wb, [2] coefficients w -> w + α (y - ŷ)x, [3] intercepts b -> b + α (y - ŷ), [4] repeat"
"mean absolute error","Error = (1/m) Σ(i=1..m) |y - ŷ|"
"mean squared error","Error = (1/2m) Σ(i=1..m) (y - ŷ)²"
"mini-batch gradient descent","Split data into many batches, and apply gradient descent."
"linear regression","Regress supporting linear space features. E.g., ŷ = w₁x + w₂"
"polynomial regression","Regression supporting polynomial space features. E.g., ŷ = w₁x³ + w₂x² + w₃x + w₄"
"regularization","factor in the complexity of the model into the error, in order to avoid overfitting the training data"
"L1 Regularization","Regularization where add the absolute value of the coefficients to the error"
"L2 Regularization","Regularization where add the squares of the coefficients to the error"
"feature scaling","converting data into common range of values"
"standardizing","Feature scaling where for each value, subtract mean and divide by standard deviation"
"normalizing","Feature scaling where for each value, data scaled between 0 and 1"
"step function","converts continuous number to 0 or 1"
"Formula for entropy","- Σ(i=1..m) pᵢ log₂(pᵢ)"
"Formula for information gain","Entropy(parent) - [p₁ Entropy(child₁) + p₂ Entropy(child₂)]"
"paired data (in pandas)","the features at `X.loc[i]` is associated with the label at `y.loc[i]"
"One-hot encoding","Convert categorical data to columns of numeric data made up of 0s and 1s"
"ham (messaging)","A message that's not spam."
"Why is Naive Bayes naive?","Because it assumes independence: P(A ∩ B) = P(A)P(B)"
"Bag of Words (BoW)","count the frequency of the words in given text"
"Multinomial Naive Bayes","Naive Bayes algorithm suitable for classification with discrete features"
"Gaussian Naive Bayes","Naive Bayes algorithm suitable for classification with continuous features"
"Accuracy","ratio of the number of correct predictions to the total number of predictions"
"Precision","ratio of true positives to all positives"
"Recall (sensitivity)","ratio of true positives to all the words that were positives (true positives + false negatives)"
"F1 score","weighted average of the precision and recall"

# Dimensionality Reduction and PCA

## 1. Video: Introduction

## 2. Video: Lesson Topics

## 3. Text: Lesson Topics

## 4. Video: Latent Features

* **latent feature**: features not directly observed, but underlies observed features (e.g. size of house, quality of neighborhood)

## 5. Latent Features

## 6. Video: How to Reduce Features?

* PCA helps with the "curse of dimensionality"

* **feature reduction** (identifying subset of data that is most useful) vs **feature extraction** (constructing new latent features from original dataset)

* Two types of feature reduction:
    - **filter methods**: discerning inherent correlations between feature data in unsupervised learning, or between features and output values in supervised learning. E.g., Pearson's Correlation, LDA, ANOVA.
    - **wrapper methods**: directly select features based on their impact on the model. computationally expensive. E.g., Forward Search, Backwards Search, Recursive Feature Elimination

* Three methods of feature extraction we'll cover:
    - Principal Component Analysis (PCA)
    - Independent Component Analysis (ICA)
    - Random Projection

## 7. Video: Dimensionality Reduction

## 8. Video: PCA Properties

## 9. Quiz: How Does PCA Work?

## 10. Screencast: PCA

## 11. Notebook: PCA - Your Turn

## 12. Screencast: PCA Solution

## 13. Screencast: Interpret PCA Results

## 14. Notebook: Interpretation

## 15. Screencast: Interpretation Solution

## 16. Text: What are EigenValues & EigenVectors?

## 17. Video: When to Use PCA?

## 18. Video: Recap

## 19. Notebook: Mini-Project

## 20. Mini-Project Solution

## 21. Video: Outro

## 22. Text: Recap
